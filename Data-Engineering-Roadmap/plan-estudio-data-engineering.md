# ğŸ“ Plan de Estudio â€” Data Engineering Roadmap

## Resumen Ejecutivo

Este curso integral guÃ­a a estudiantes desde los fundamentos de ingenierÃ­a de datos hasta la construcciÃ³n de pipelines de producciÃ³n completos. Combina teorÃ­a sÃ³lida con prÃ¡ctica intensiva, cubriendo SQL avanzado, Python para ETL, transformaciones con dbt, orquestaciÃ³n con Airflow, y modelado de datos en la nube.

**Nivel:** Principiante a Intermedio  
**DuraciÃ³n:** 8-12 semanas (10-15 horas/semana)  
**Modalidad:** Auto-guiado con ejercicios verificables  
**Proyecto Final:** Pipeline end-to-end desplegado en cloud

## ğŸ¯ Objetivos de Aprendizaje

Al completar este roadmap serÃ¡s capaz de:

1. DiseÃ±ar y construir pipelines de datos escalables
2. Escribir SQL optimizado para grandes volÃºmenes de datos
3. Desarrollar procesos ETL/ELT con Python y dbt
4. Orquestar workflows complejos con Apache Airflow
5. Modelar datos usando esquemas dimensionales y relacionales
6. Implementar testing, monitoreo y observabilidad
7. Desplegar soluciones en plataformas cloud (AWS/Azure/GCP)
8. Aplicar mejores prÃ¡cticas de ingenierÃ­a de software a datos

## ğŸ“š Estructura del Curso

### **MÃ³dulo 01: IntroducciÃ³n a Data Engineering**
- ğŸ¯ Rol del Data Engineer en organizaciones modernas
- ğŸ”§ ConfiguraciÃ³n del entorno de desarrollo
- ğŸ³ Docker y contenedores para desarrollo local
- â±ï¸ DuraciÃ³n: 4-6 horas

### **MÃ³dulo 02: SQL para Data Engineering**
- ğŸ“Š Consultas avanzadas: CTEs, window functions, optimizaciÃ³n
- ğŸ”„ Operaciones MERGE, UPSERT y deduplicaciÃ³n
- ğŸš€ Performance tuning e Ã­ndices
- â±ï¸ DuraciÃ³n: 12-15 horas

### **MÃ³dulo 03: Python para Data Engineering**
- ğŸ Ingesta de datos desde APIs y bases de datos
- ğŸ”„ Transformaciones con pandas y procesamiento batch
- ğŸ“¦ Manejo de dependencias y entornos virtuales
- â±ï¸ DuraciÃ³n: 10-12 horas

### **MÃ³dulo 04: Transformaciones con dbt**
- ğŸ¨ Models, tests, y documentaciÃ³n
- ğŸ”„ Flujo dev â†’ staging â†’ production
- ğŸ“Š Materializations (table, view, incremental)
- â±ï¸ DuraciÃ³n: 10-12 horas

### **MÃ³dulo 05: OrquestaciÃ³n con Airflow**
- ğŸ”€ DAGs, operators y sensores
- ğŸ“… Scheduling y dependencias
- ğŸ” Monitoreo y troubleshooting
- â±ï¸ DuraciÃ³n: 12-15 horas

### **MÃ³dulo 06: Data Warehousing con Snowflake**
- â„ï¸ Arquitectura multi-cluster y virtual warehouses
- ğŸ” Roles, permisos y governance
- ğŸ’° OptimizaciÃ³n de costos y performance
- â±ï¸ DuraciÃ³n: 8-10 horas

### **MÃ³dulo 07: Modelado Dimensional**
- â­ DiseÃ±o de star schema y snowflake schema
- ğŸ“… Slowly Changing Dimensions (SCD)
- ğŸ¯ Fact tables y mÃ©tricas calculadas
- â±ï¸ DuraciÃ³n: 8-10 horas

### **MÃ³dulo 08: Modelado Relacional**
- ğŸ”— NormalizaciÃ³n (1NF, 2NF, 3NF)
- ğŸ”‘ Primary keys, foreign keys e integridad referencial
- ğŸ“ DiseÃ±o de esquemas transaccionales
- â±ï¸ DuraciÃ³n: 6-8 horas

### **MÃ³dulo 09: Testing y DepuraciÃ³n**
- âœ… Tests unitarios, integraciÃ³n y data quality
- ğŸ› Debugging de pipelines y anÃ¡lisis de logs
- ğŸ“Š Great Expectations y validaciones
- â±ï¸ DuraciÃ³n: 8-10 horas

### **MÃ³dulo 10: Pipelines en Cloud**
- â˜ï¸ AWS (S3, Glue, Redshift) / Azure (Synapse, Data Factory) / GCP (BigQuery, Dataflow)
- ğŸš€ CI/CD para pipelines de datos
- ğŸ”„ IaC con Terraform
- â±ï¸ DuraciÃ³n: 12-15 horas

### **MÃ³dulo 11: Buenas PrÃ¡cticas y Observabilidad**
- ğŸ“ Logging estructurado y monitoreo
- ğŸ”” Alertas y SLAs
- ğŸ”’ Seguridad y compliance (GDPR, encriptaciÃ³n)
- â±ï¸ DuraciÃ³n: 6-8 horas

### **MÃ³dulo 12: Proyecto Integrador**
- ğŸ¯ Pipeline completo: ingestiÃ³n â†’ transformaciÃ³n â†’ visualizaciÃ³n
- ğŸ“¦ Entregables: cÃ³digo, documentaciÃ³n, presentaciÃ³n
- ğŸš€ Despliegue en producciÃ³n (cloud)
- â±ï¸ DuraciÃ³n: 15-20 horas

## ğŸ“ Formato de cada MÃ³dulo

Cada mÃ³dulo incluye 5 archivos estandarizados:

1. **README.md**: Objetivos, contenido teÃ³rico, actividades prÃ¡cticas, duraciÃ³n
2. **actividad-interactiva.md**: 6-10 ejercicios prÃ¡cticos con:
   - Objetivo claro y contexto
   - Pasos detallados con comandos verificables
   - Campos para completar respuestas
   - DuraciÃ³n estimada por ejercicio
   - Criterios de validaciÃ³n
3. **progreso.md**: Checklist de avance con casillas marcables
4. **retroalimentacion.md**: RÃºbrica de evaluaciÃ³n con porcentajes
5. **recursos.md**: Enlaces, datasets, herramientas, documentaciÃ³n oficial

## ğŸ› ï¸ Prerequisitos

### Conocimientos Previos
- âœ… ProgramaciÃ³n bÃ¡sica (cualquier lenguaje)
- âœ… LÃ­nea de comandos/terminal (bÃ¡sico)
- âœ… Conceptos de bases de datos (deseable)
- âœ… Git bÃ¡sico (clone, commit, push)

### Herramientas Necesarias
- ğŸ’» Laptop/PC con 8GB RAM mÃ­nimo
- ğŸ³ Docker Desktop instalado
- ğŸ”§ Git instalado
- ğŸ“ Editor de cÃ³digo (VSCode recomendado)
- â˜ï¸ Cuenta gratuita en Snowflake/AWS/GCP (para mÃ³dulos cloud)

## ğŸš€ CÃ³mo Usar este Roadmap

### Paso 1: ConfiguraciÃ³n Inicial
```bash
# Clonar el repositorio
git clone https://github.com/angra8410/all-my-learnings.git
cd all-my-learnings/Data-Engineering-Roadmap

# Crear rama de trabajo personal
git checkout -b feature/mi-progreso-data-eng
```

### Paso 2: Seguir el Orden Secuencial
- Comienza siempre por el **README.md** del mÃ³dulo
- Lee el contenido teÃ³rico
- Completa los ejercicios de **actividad-interactiva.md**
- Marca tu progreso en **progreso.md**
- Revisa tus respuestas con **retroalimentacion.md**
- Consulta **recursos.md** para profundizar

### Paso 3: Ritmo Recomendado
- **Intensivo**: 2 mÃ³dulos/semana (20+ horas/semana) â†’ 6 semanas
- **Moderado**: 1 mÃ³dulo/semana (10-15 horas/semana) â†’ 12 semanas
- **Relajado**: 1 mÃ³dulo cada 2 semanas (5-8 horas/semana) â†’ 24 semanas

### Paso 4: Proyecto Integrador
Una vez completados los mÃ³dulos 01-11, dedica tiempo al proyecto final. Este proyecto consolidarÃ¡ todos los conceptos y te darÃ¡ un portafolio demostrable.

## ğŸ“Š Sistema de EvaluaciÃ³n

Cada mÃ³dulo tiene criterios definidos en `retroalimentacion.md`:

- **Completitud**: Â¿Se completaron todos los ejercicios? (30%)
- **Calidad tÃ©cnica**: Â¿El cÃ³digo funciona y sigue mejores prÃ¡cticas? (40%)
- **DocumentaciÃ³n**: Â¿EstÃ¡n documentadas las decisiones y procesos? (20%)
- **Creatividad**: Â¿Se agregaron mejoras o casos adicionales? (10%)

**Aprobado**: â‰¥70% | **Excelente**: â‰¥90%

## ğŸ“ CertificaciÃ³n y Siguientes Pasos

Al completar este roadmap:

1. âœ… TendrÃ¡s un repositorio con 12+ proyectos prÃ¡cticos
2. âœ… Portfolio demostrable para entrevistas
3. âœ… Base sÃ³lida para certificaciones:
   - Snowflake SnowPro Core
   - AWS Certified Data Analytics
   - Google Professional Data Engineer
   - dbt Analytics Engineering Certification

### Recursos Adicionales Post-Curso
- ğŸ“š Libros: "Designing Data-Intensive Applications" (Kleppmann)
- ğŸ¥ Cursos avanzados: Databricks, Spark, Kafka
- ğŸ¤ Comunidades: dbt Community, DataTalks.Club, Data Engineering Slack
- ğŸ“ Blogs: Seattle Data Guy, Data Engineering Weekly

## ğŸ¤ Contribuciones y Feedback

Este roadmap es un recurso vivo. Si encuentras errores, quieres sugerir mejoras o agregar recursos:

1. Abre un Issue en el repositorio
2. PropÃ³n cambios mediante Pull Request
3. Comparte tu experiencia y feedback

## ğŸ“ Soporte

- **GitHub Issues**: Para reportar errores o sugerencias
- **Discussions**: Para preguntas generales del curso
- **README de cada mÃ³dulo**: Incluye recursos especÃ­ficos de ayuda

---

**Â¡Bienvenido al mundo de Data Engineering! ğŸš€**  
*Este journey transformarÃ¡ tu carrera en datos. Â¡Comencemos!*