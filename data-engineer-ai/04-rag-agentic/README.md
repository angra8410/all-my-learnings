# MÃ³dulo 4: RAG y Sistemas Agentic

## IntroducciÃ³n

En este mÃ³dulo aprenderÃ¡s a construir sistemas RAG (Retrieval Augmented Generation), la arquitectura mÃ¡s importante para aplicaciones de IA empresariales. TambiÃ©n explorarÃ¡s sistemas agentic que combinan LLMs con herramientas.

## Objetivos del MÃ³dulo

Al finalizar este mÃ³dulo, serÃ¡s capaz de:

- ðŸŽ¯ Entender la arquitectura RAG
- ðŸŽ¯ Generar embeddings con OpenAI/Hugging Face
- ðŸŽ¯ Trabajar con bases de datos vectoriales (Pinecone, Chroma)
- ðŸŽ¯ Implementar bÃºsqueda semÃ¡ntica
- ðŸŽ¯ Construir un sistema RAG completo
- ðŸŽ¯ IntroducciÃ³n a agentes y tools

## Â¿Por quÃ© es importante?

RAG es la soluciÃ³n a las "alucinaciones" de los LLMs. Permite que los modelos accedan a informaciÃ³n actualizada y especÃ­fica de tu dominio sin reentrenar.

## Conceptos Principales

### 1. Arquitectura RAG

```
Usuario â†’ Query
    â†“
Convertir query a embedding (vector)
    â†“
Buscar en base vectorial (similarity search)
    â†“
Recuperar top-k documentos relevantes
    â†“
Combinar documentos + query en prompt
    â†“
LLM genera respuesta basada en contexto
    â†“
Respuesta al usuario
```

### 2. Embeddings

**Â¿QuÃ© son?**
- RepresentaciÃ³n numÃ©rica de texto
- Capturan significado semÃ¡ntico
- Vectores de alta dimensiÃ³n (ej: 1536 dimensiones)

**GeneraciÃ³n con OpenAI**:
```python
from openai import OpenAI

client = OpenAI(api_key="tu-key")

def get_embedding(text: str) -> list:
    response = client.embeddings.create(
        model="text-embedding-3-small",
        input=text
    )
    return response.data[0].embedding

# Uso
embedding = get_embedding("Data engineering is important")
print(f"Vector de {len(embedding)} dimensiones")
```

**Con Hugging Face (gratis)**:
```python
from sentence_transformers import SentenceTransformer

model = SentenceTransformer('all-MiniLM-L6-v2')
embedding = model.encode("Data engineering is important")
print(f"Vector de {len(embedding)} dimensiones")
```

### 3. Bases de Datos Vectoriales

**Pinecone (Cloud)**:
```python
import pinecone

pinecone.init(api_key="tu-key")
index = pinecone.Index("mi-index")

# Insertar vectores
index.upsert(vectors=[
    ("id1", embedding1, {"text": "texto original"}),
    ("id2", embedding2, {"text": "otro texto"})
])

# BÃºsqueda
results = index.query(
    vector=query_embedding,
    top_k=5,
    include_metadata=True
)
```

**Chroma (Local)**:
```python
import chromadb

client = chromadb.Client()
collection = client.create_collection("docs")

# Agregar documentos
collection.add(
    documents=["doc1", "doc2"],
    metadatas=[{"source": "file1"}, {"source": "file2"}],
    ids=["id1", "id2"]
)

# BÃºsqueda
results = collection.query(
    query_texts=["query"],
    n_results=5
)
```

### 4. Sistema RAG con LangChain

```python
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA

# 1. Crear vector store
embeddings = OpenAIEmbeddings()
vectorstore = Chroma.from_documents(
    documents=chunks,
    embedding=embeddings
)

# 2. Crear retriever
retriever = vectorstore.as_retriever(
    search_kwargs={"k": 3}
)

# 3. Crear chain
llm = ChatOpenAI(model="gpt-3.5-turbo")
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True
)

# 4. Hacer pregunta
result = qa_chain({"query": "Â¿QuÃ© es data engineering?"})
print(result['result'])
print(f"Fuentes: {result['source_documents']}")
```

### 5. Sistemas Agentic

**Â¿QuÃ© son?**
- LLM que puede usar herramientas
- Decide quÃ© herramienta usar segÃºn la tarea
- Ejemplos: calculadora, bÃºsqueda web, APIs

**Agente simple con LangChain**:
```python
from langchain.agents import initialize_agent, Tool
from langchain.agents import AgentType

# Definir herramientas
tools = [
    Tool(
        name="Calculator",
        func=lambda x: eval(x),
        description="Ãštil para cÃ¡lculos matemÃ¡ticos"
    ),
    Tool(
        name="Database",
        func=query_database,
        description="Busca informaciÃ³n en la base de datos"
    )
]

# Crear agente
agent = initialize_agent(
    tools,
    llm,
    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,
    verbose=True
)

# Usar agente
agent.run("Â¿CuÃ¡l es el promedio de ventas en Q1 2024?")
```

## ImplementaciÃ³n PrÃ¡ctica

### Sistema RAG Completo

```python
from langchain.document_loaders import DirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma
from langchain.chains import RetrievalQA
from langchain.chat_models import ChatOpenAI

class RAGSystem:
    def __init__(self, docs_path: str, persist_directory: str):
        self.docs_path = docs_path
        self.persist_directory = persist_directory
        self.vectorstore = None
        self.qa_chain = None
    
    def ingest_documents(self):
        """Carga e indexa documentos"""
        # Cargar documentos
        loader = DirectoryLoader(self.docs_path, glob="**/*.txt")
        documents = loader.load()
        
        # Dividir en chunks
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200
        )
        chunks = text_splitter.split_documents(documents)
        
        # Crear embeddings y store
        embeddings = OpenAIEmbeddings()
        self.vectorstore = Chroma.from_documents(
            chunks,
            embeddings,
            persist_directory=self.persist_directory
        )
        
        print(f"âœ… Indexados {len(chunks)} chunks")
    
    def setup_qa_chain(self):
        """Configura el chain de Q&A"""
        if not self.vectorstore:
            raise ValueError("Primero debes ingestar documentos")
        
        retriever = self.vectorstore.as_retriever(
            search_kwargs={"k": 5}
        )
        
        llm = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0
        )
        
        self.qa_chain = RetrievalQA.from_chain_type(
            llm=llm,
            retriever=retriever,
            return_source_documents=True
        )
        
        print("âœ… QA Chain configurado")
    
    def ask(self, question: str) -> dict:
        """Hace una pregunta al sistema"""
        if not self.qa_chain:
            raise ValueError("Primero debes configurar QA chain")
        
        result = self.qa_chain({"query": question})
        
        return {
            'answer': result['result'],
            'sources': [doc.metadata for doc in result['source_documents']]
        }

# Uso
rag = RAGSystem('./docs', './chroma_db')
rag.ingest_documents()
rag.setup_qa_chain()

response = rag.ask("Â¿QuÃ© es ETL?")
print(f"Respuesta: {response['answer']}")
print(f"Fuentes: {response['sources']}")
```

## Mejores PrÃ¡cticas

### 1. OptimizaciÃ³n de Retrieval
```python
# Usar reranking para mejorar resultados
from langchain.retrievers import ContextualCompressionRetriever
from langchain.retrievers.document_compressors import LLMChainExtractor

compressor = LLMChainExtractor.from_llm(llm)
compression_retriever = ContextualCompressionRetriever(
    base_compressor=compressor,
    base_retriever=retriever
)
```

### 2. Evaluation
```python
# Evaluar calidad de respuestas
from langchain.evaluation import load_evaluator

evaluator = load_evaluator("qa")
result = evaluator.evaluate_strings(
    prediction=response['answer'],
    reference="respuesta correcta",
    input=question
)
```

### 3. Caching
```python
# Cache para queries frecuentes
from functools import lru_cache

@lru_cache(maxsize=100)
def cached_embedding(text: str):
    return get_embedding(text)
```

## De Open Source a Enterprise

| Aspecto | Open Source | Enterprise |
|---------|-------------|------------|
| **Vector DB** | Chroma, FAISS | Pinecone, Weaviate Cloud |
| **Embeddings** | Sentence Transformers | OpenAI, Cohere |
| **LLM** | Llama 2, Mistral | GPT-4, Claude |
| **Hosting** | Self-hosted | Managed services |

**Transferencia**: La arquitectura RAG es la misma, solo cambian las herramientas.

## Conceptos Clave

- ðŸ”‘ **RAG**: Retrieval Augmented Generation
- ðŸ”‘ **Embeddings**: Vectores que representan significado
- ðŸ”‘ **Vector DB**: Base de datos para bÃºsqueda semÃ¡ntica
- ðŸ”‘ **Similarity Search**: Encontrar documentos similares
- ðŸ”‘ **Agentes**: LLMs que usan herramientas

## PrÃ³ximos Pasos

En el **MÃ³dulo 5: AWS Bedrock** aprenderÃ¡s:
- Usar modelos foundation en AWS
- Implementar RAG serverless
- IntegraciÃ³n con servicios AWS
- GestiÃ³n de costos

## Recursos Adicionales

- ðŸ“– [LangChain RAG Docs](https://python.langchain.com/docs/use_cases/question_answering/)
- ðŸ“– [Pinecone Learning Center](https://www.pinecone.io/learn/)
- ðŸŽ¥ [RAG Tutorial](https://www.youtube.com/watch?v=example)
- ðŸ“„ [RAG Paper](https://arxiv.org/abs/2005.11401)

---

**Â¡Excelente trabajo completando el MÃ³dulo 4!** ðŸŽ‰

Ya sabes construir sistemas RAG. ContinÃºa a [actividad-interactiva.md](actividad-interactiva.md).
