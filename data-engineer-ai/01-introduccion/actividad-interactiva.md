# Actividades Interactivas - M√≥dulo 1: Introducci√≥n al Data Engineering para IA

## Secci√≥n 1: Preguntas de Opci√≥n M√∫ltiple

### Pregunta 1
**¬øCu√°l es la responsabilidad principal de un Data Engineer?**

A) Crear modelos de Machine Learning  
B) Dise√±ar y mantener sistemas que recolectan, almacenan y procesan datos  
C) Hacer an√°lisis estad√≠stico y visualizaciones  
D) Gestionar bases de datos √∫nicamente

**Tu respuesta**: B

---

### Pregunta 2
**¬øQu√© significa ETL?**

A) Execute, Test, Load  
B) Extract, Transform, Load  
C) Evaluate, Train, Learn  
D) Export, Transfer, Link

**Tu respuesta**: B

---

### Pregunta 3
**¬øCu√°l es la diferencia principal entre Data Warehouse y Data Lake?**

A) Data Warehouse es m√°s caro que Data Lake  
B) Data Warehouse almacena datos estructurados con esquema definido, Data Lake almacena datos raw en cualquier formato  
C) Data Lake es solo para big data, Data Warehouse para datos peque√±os  
D) No hay diferencia real, son t√©rminos intercambiables

**Tu respuesta**: B

---

### Pregunta 4
**¬øQu√© caracteriza al procesamiento en tiempo real (streaming)?**

A) Procesa grandes vol√∫menes una vez al d√≠a  
B) Procesa datos continuamente a medida que llegan  
C) Es m√°s barato que procesamiento batch  
D) Solo funciona con Apache Spark

**Tu respuesta**: B

---

### Pregunta 5
**¬øCu√°l de estas herramientas NO es t√≠picamente usada por Data Engineers?**

A) Apache Airflow  
B) SQL  
C) Adobe Photoshop  
D) Pandas

**Tu respuesta**: C

---

### Pregunta 6
**¬øQu√© es un Lakehouse?**

A) Una casa junto a un lago donde trabajan data engineers  
B) Una combinaci√≥n de Data Lake y Data Warehouse  
C) Un tipo de base de datos NoSQL  
D) Una versi√≥n antigua de Data Lake

**Tu respuesta**: B

---

### Pregunta 7
**¬øQu√© porcentaje del tiempo de un Data Engineer t√≠picamente se dedica a preparaci√≥n de datos?**

A) 20%  
B) 40%  
C) 60%  
D) 80%

**Tu respuesta**: D

---

### Pregunta 8
**¬øCu√°l es la funci√≥n principal de Apache Airflow?**

A) Procesar big data  
B) Orquestar y programar workflows de datos  
C) Almacenar datos  
D) Crear visualizaciones

**Tu respuesta**: B

---

## Secci√≥n 2: Verdadero o Falso

Marca V (Verdadero) o F (Falso) para cada afirmaci√≥n:

1. **Un Data Engineer y un Data Scientist hacen exactamente el mismo trabajo.** F

2. **SQL es un lenguaje esencial para Data Engineers.** V

3. **Los Data Engineers solo trabajan con datos estructurados.** F

4. **Los sistemas de IA modernos pueden funcionar sin Data Engineers.** F

5. **Procesamiento batch es siempre mejor que streaming.** F

6. **Python es el lenguaje m√°s usado en Data Engineering.** V

7. **Data quality es responsabilidad √∫nicamente del equipo de QA.** F

8. **Los pipelines de datos necesitan ser monitoreados continuamente.** V

---

## Secci√≥n 3: Relaciona Conceptos

Conecta cada t√©rmino con su descripci√≥n correcta:

**T√©rminos:**
1. Data Lake
2. Apache Airflow
3. ETL
4. Data Warehouse
5. Streaming

**Descripciones:**
A) Orquestador de workflows de datos  
B) Almac√©n de datos estructurados optimizado para an√°lisis  
C) Procesamiento de datos en tiempo real  
D) Almacenamiento de datos raw en cualquier formato  
E) Extract, Transform, Load

**Tus respuestas:**
1 ‚Üí D | 2 ‚Üí A | 3 ‚Üí E | 4 ‚Üí B | 5 ‚Üí C

---

## Secci√≥n 4: Completar el C√≥digo

### Ejercicio 1
Completa el pipeline ETL b√°sico:

```python
import pandas as pd

def extract():
    """Extrae datos de una fuente"""
    data = {
        'nombre': ['Ana', 'Luis', 'Mar√≠a'],
        'edad': [25, 30, 28]
    }
    df = pd.DataFrame(data)  # Completa aqu√≠
    return df

def transform(df):
    """Agrega una columna calculada"""
    df['edad_en_5_a√±os'] = df['edad'] + 5  # Completa aqu√≠
    return df

def load(df):
    """Guarda los datos"""
    df.to_csv(Filename= index=False)  # Completa el nombre
    print("Datos guardados")

if __name__ == "__main__":
    print("üöÄ Iniciando pipeline ETL...")

# Ejecutar pipeline
raw_data = extract()
clean_data = transform(raw_data)
load(clean_data)
```

---

### Ejercicio 2
Completa la validaci√≥n de datos:

```python
def validate_email(email: str) -> bool:
    """Valida que un email sea v√°lido"""
    return '@' in email and '.' in email  # Completa aqu√≠

def clean_data(df):
    """Limpia DataFrame eliminando valores nulos"""
    return df.dropna()  # Completa con m√©todo de pandas

# Test
assert validate_email("user@example.com") == True
assert validate_email("invalid-email") == False  # Completa aqu√≠
```

---

## Secci√≥n 5: An√°lisis de Casos

### Caso 1: E-commerce en Crecimiento

**Contexto:**
Una tienda online procesa 10,000 pedidos diarios. Necesitan:
- Reportes de ventas actualizados cada noche
- Detecci√≥n de fraude en tiempo real
- Recomendaciones de productos

**Preguntas:**

1. **¬øQu√© tipo de procesamiento usar√≠as para los reportes de ventas?**
   - [x ] Batch (una vez al d√≠a)
   - [ ] Streaming (tiempo real)
   
   **Justifica**: Batch, porque ellos necesitan los reportes todos los dias en la noche.
   
3. **¬øY para detecci√≥n de fraude?**
   - [ ] Batch
   - [x] Streaming
   
   **Justifica**: Se necesita monitorizar transacciones fraudulentas en tiempo real.

4. **¬øQu√© tecnolog√≠as recomendar√≠as para cada necesidad?**
   - Reportes: power bi, snowflake
   - Fraude: kafka, flink, redis, seldon, snowflake
   - Recomendaciones: ___________________________________________

---

### Caso 2: Sistema RAG para Documentaci√≥n

**Contexto:**
Una empresa quiere crear un chatbot que responda preguntas sobre sus manuales t√©cnicos (1000+ documentos PDF).

**Preguntas:**

1. **¬øQu√© pasos del pipeline de datos son necesarios?**
   
   Ordena del 1 al 6:
   - [5] Generar embeddings
   - [1] Extraer texto de PDFs
   - [2] Almacenar en base vectorial
   - [4] Dividir texto en chunks
   - [3] Limpiar y normalizar texto
   - [6] Implementar b√∫squeda sem√°ntica

2. **¬øQu√© rol juega el Data Engineer aqu√≠?**
   
   Es quien Extrae, almacena, hace data transformation, modela, sirve el mod√®lo e itera cuantas veces sea necesario.
   ___________________________________________

3. **¬øBatch o streaming para este caso?**
   
   Batch

---

## Secci√≥n 6: Dise√±o de Arquitectura

### Ejercicio: Tu Primer Dise√±o

**Escenario:**
Dise√±a una arquitectura simple para este caso:

**Requisito**: Una app de an√°lisis de redes sociales que:
1. Recolecta tweets sobre un tema cada hora
2. Analiza sentimiento (positivo/negativo)
3. Genera dashboard con tendencias

**Tu dise√±o (dibuja o describe):**

```
INGESTA:
¬øDe d√≥nde vienen los datos?
Los datos se obtienen desde X a trav√®s de la API de ellos

ALMACENAMIENTO:
¬øD√≥nde los guardas?
S3

PROCESAMIENTO:
¬øC√≥mo los procesas?
Se hace an√†lisis de sentimiento, se buscan las palabras y se hace un chunk de lo que se necesita para preparar los datos en vectores

VISUALIZACI√ìN:
¬øC√≥mo los muestras?
Yo los muestro a trav√®s de una herramienta de BI

HERRAMIENTAS:
¬øQu√© tecnolog√≠as usar√≠as?
Airflow, power bi, kafka
```

---

## Secci√≥n 7: Ejercicios Pr√°cticos de C√≥digo

### Ejercicio 1: Pipeline ETL Completo

**Tarea**: Crea un pipeline que procese datos de ventas.

**Archivo**: `ventas_etl.py`

```python
import pandas as pd
from datetime import datetime

# Datos de ejemplo
ventas_raw = {
    'fecha': ['2024-01-01', '2024-01-02', '2024-01-01'],
    'producto': ['laptop', 'MOUSE', 'Teclado  '],
    'precio': [1000, 25, None],
    'cantidad': [2, 10, 5]
}

# TODO: Implementa las siguientes funciones

def extract_data(data_dict):
    """Convierte dict a DataFrame"""
    df = pd.DataFrame(data_dict)
    pass

def transform_data(df):
    """
    Aplica las siguientes transformaciones:
    1. Normalizar nombres de productos (lowercase, sin espacios extra)
    2. Llenar precios None con 0
    3. Calcular columna 'total' = precio * cantidad
    4. Convertir fecha a datetime
    """
    df[normalizar_nombres] = df['nombre'].str.lower().str.replace(" ", "", regex=False)
    df[llenar precios None con 0] = df['precio'].fillna(0)
    df[total] = df[precio] * df[cantidad]
    df[convertirfecha] = pd.to_datetime(df['fecha'])
    pass

def validate_data(df):
    """
    Valida que:
    1. No hay precios negativos
    2. Cantidad es mayor que 0
    3. Producto no es string vac√≠o
    Retorna True si pasa todas las validaciones
    """
    
    df[nopreciosnegativos] = df[precios]>=0
    df[cantidadmayorquecero]= df[cantidad]>0
    df['productonovacio'] = df['producto'].notna() & df['producto'].astype(str).str.strip().ne('')
    df['valid'] = df['nopreciosnegativos'] & df['cantidadmayorquecero'] & df['productonovacio']
    return df['valid'].all()

    pass

def load_data(df, filename='ventas_clean.csv'):
    """Guarda en CSV"""
    df.to_csv(filename, index=False)
    print(f"‚úÖ Datos cargados en {filename}")
    pass

# Ejecutar pipeline
if __name__ == "__main__":
    print("üöÄ Iniciando pipeline ETL...")
    
    # ETL
    raw_data = extract()
    transformed_data = transform(raw_data)
    load(transformed_data)
    
    print("‚ú® Pipeline completado exitosamente!")
```

**Bonus**: Agrega logging para saber qu√© hace cada paso.
import logging
import pandas as pd

def validate_simple(df: pd.DataFrame,
                    price_col: str = 'precio',
                    qty_col: str = 'cantidad',
                    prod_col: str = 'producto') -> bool:
    """
    Validaci√≥n simple:
      1) precio >= 0
      2) cantidad > 0
      3) producto no es cadena vac√≠a ni NaN

    Devuelve True si TODAS las filas cumplen las 3 reglas.
    Imprime conteos simples de filas que fallan cada regla.
    """
    # Logging muy simple
    logging.basicConfig(level=logging.INFO, format="%(message)s")
    logger = logging.getLogger("validate_simple")

    # Trabajamos con copia para no mutar el original
    df = df.copy()

    # Convertir columnas num√©ricas (no num√©rico -> NaN)
    prices = pd.to_numeric(df[price_col], errors='coerce')
    qty = pd.to_numeric(df[qty_col], errors='coerce')

    # Producto como string y strip() para quitar espacios
    prod = df[prod_col].astype(str).str.strip()

    # M√°scaras de validaci√≥n
    mask_price = prices >= 0
    mask_qty = qty > 0
    mask_prod = df[prod_col].notna() & (prod != '')

    # Conteos e info simple
    total = len(df)
    n_price_bad = (~mask_price).sum()
    n_qty_bad = (~mask_qty).sum()
    n_prod_bad = (~mask_prod).sum()

    logger.info(f"Total filas: {total}")
    logger.info(f"Filas con precio negativo: {int(n_price_bad)}")
    logger.info(f"Filas con cantidad <= 0 o no num√©rica: {int(n_qty_bad)}")
    logger.info(f"Filas con producto vac√≠o o NaN: {int(n_prod_bad)}")

    # Resultado combinado: True si todas las filas son v√°lidas
    valid = mask_price & mask_qty & mask_prod
    return valid.all()
---

### Ejercicio 2: An√°lisis de Logs

**Tarea**: Procesa logs de una aplicaci√≥n web.

**Archivo**: `log_processor.py`

```python
import pandas as pd

# Logs de ejemplo
logs = """
2024-01-15 10:30:15 INFO User login successful - user_id: 123
2024-01-15 10:30:20 ERROR Database connection failed - retrying...
2024-01-15 10:30:45 INFO User 456 viewed product page
2024-01-15 10:31:10 WARNING High memory usage: 85%
2024-01-15 10:31:25 INFO Purchase completed - order_id: 789
"""

# TODO: Implementa
def parse_logs(log_string):
    """
    Convierte logs a DataFrame con columnas:
    - timestamp
    - level (INFO, ERROR, WARNING)
    - message
    """
    # Tu c√≥digo aqu√≠
    pass

def analyze_logs(df):
    """
    Calcula:
    - Total de logs por nivel
    - N√∫mero de errores
    - Primera y √∫ltima entrada
    """
    # Tu c√≥digo aqu√≠
    pass

# Ejecutar
if __name__ == "__main__":
    # Tu c√≥digo aqu√≠
    pass
```

---

### Ejercicio 3: Configuraci√≥n de Entorno

**Tarea**: Configura tu entorno de desarrollo completo.

**Checklist t√©cnico:**
- [x] Python 3.10+ instalado
- [x] Entorno virtual creado
- [x] pandas, numpy, jupyter instalados
- [x] Docker instalado y funcionando
- [x] Git configurado
- [x] Cuenta GitHub creada
- [ ] Cuenta AWS Free Tier creada (opcional para m√≥dulos futuros)

**Verifica tu setup:**

```bash
# Crea archivo verify_setup.py
python verify_setup.py
```

```python
# verify_setup.py
import sys

def check_python_version():
    version = sys.version_info
    if version.major >= 3 and version.minor >= 10:
        print("‚úÖ Python version OK:", sys.version)
    else:
        print("‚ùå Python version too old. Need 3.10+")

def check_packages():
    required = ['pandas', 'numpy', 'requests']
    for package in required:
        try:
            __import__(package)
            print(f"‚úÖ {package} installed")
        except ImportError:
            print(f"‚ùå {package} NOT installed")

if __name__ == "__main__":
    print("üîç Verificando setup...\n")
    check_python_version()
    check_packages()
    print("\n‚ú® Verificaci√≥n completa!")
```

---

## Secci√≥n 8: Investigaci√≥n y Pensamiento Cr√≠tico

### Pregunta 1: Investigaci√≥n de Herramientas

**Tarea**: Investiga y compara dos orquestadores.

| Caracter√≠stica | Apache Airflow | Prefect |
|----------------|----------------|---------|
| A√±o de creaci√≥n | 2014 | 2018 |
| Lenguaje | Python (DAGs en Python) | Python (Flows/Tasks en Python)
| Ventajas | - Muy maduro y ampliamente adoptado; gran ecosistema de operadores/integraciones. | ___ |
| Desventajas | ___ | ___ |
| Casos de uso ideales | Airbnb | Perfect |

**¬øCu√°l elegir√≠as para un proyecto personal y por qu√©?**
Airflow, me parece que encajaria mas en lo que tengo pensado hacer, y tambien por la madurez que ya tiene, mas de una decada.
___________________________________________

---

### Pregunta 2: An√°lisis de Arquitectura Real

**Tarea**: Investiga la arquitectura de datos de Netflix.

1. **¬øQu√© problemas de datos tiene Netflix?**
   Escala masiva: ingesta, almacenamiento y procesamiento de petabytes de eventos por d√≠a (logs, m√©tricas, eventos de reproducci√≥n).
Latencia/consistencia: necesidad de respuestas en tiempo real para personalizaci√≥n, recomendaciones y enrutamiento.
Heterogeneidad de datos: eventos en tiempo real, telemetr√≠a, logs, m√©tricas, datos transaccionales y datasets de entrenamiento.
Calidad y gobernanza: asegurar calidad, trazabilidad y metadata (linaje, esquemas) para ML/BI en un ecosistema distribuido.
Evoluci√≥n de esquemas: cambios frecuentes en eventos/telemetr√≠a deben gestionarse sin romper consumidores.
Disponibilidad y multi‚Äëregi√≥n: replicaci√≥n y tolerancia a fallos para servir a usuarios globales.
Observabilidad y debugging: gran volumen requiere buenas herramientas para monitoreo, alertas y root‚Äëcause analysis.
Costos: optimizar almacenamiento y c√≥mputo en la nube (S3, EMR, etc.) para workloads muy grandes.

2. **¬øQu√© tecnolog√≠as usa?** (Investiga en internet)
   Cloud / almacenamiento:
Amazon Web Services: S3 (data lake), EC2, EMR, Lambda (usos varios).
Ingesta / transporte de eventos:
Suro (Netflix OSS) ‚Äî sistema de transporte/ingesta de eventos; y uso de sistemas de streaming como Apache Kafka (o soluciones internas/h√≠bridas) seg√∫n necesidad.
Procesamiento en streaming / near‚Äëreal‚Äëtime:
Mantis (Netflix OSS) ‚Äî plataforma de stream processing en tiempo real; tambi√©n usan frameworks como Flink/Spark Streaming en ciertos casos.
Procesamiento batch / ML:
Apache Spark (sobre EMR) para ETL y entrenamiento; Hadoop/Hive hist√≥ricamente (migraci√≥n de HDFS ‚Üí S3).
Query y an√°litica interactiva:
Presto (ahora Trino en otros sitios, pero Netflix impulsa Presto) para consultas interactivas a gran escala.
Orquestaci√≥n y ejecuci√≥n:
Genie (Netflix OSS) para orquestar jobs; adem√°s control propio sobre scheduling y runners.
Almacenamiento de estado / bases:
Cassandra (uso en casos de almacenamiento NoSQL), MySQL y caches como EVCache (memcached‚Äëbased) para baja latencia.
Metadata / gobernanza:
Metacat (Netflix OSS) y otros servicios internos para cat√°logo/metadata/linaje.
Observabilidad / monitoring:
Atlas (metrics, Netflix OSS), Spinnaker (deploy), herramientas internas de logging y dashboards.
Formatos y herramientas del ecosistema:
Parquet/Avro/ORC para datos columnados; Jupyter/Zeppelin para exploraci√≥n; Python stack (pandas, numpy), frameworks ML (TensorFlow / PyTorch u otros seg√∫n equipo).
Infraestructura contenedorizada / orquestaci√≥n:
Titus (Netflix OSS) para contenedores; Kubernetes en algunos contextos.
Nota: Netflix adem√°s aporta muchos proyectos OSS (Suro, Mantis, Genie, Metacat, EVCache, Atlas, Titus, Spinnaker) ‚Äî la adopci√≥n exacta de cada componente puede variar con el tiempo.

3. **¬øBatch o streaming?**
   Ambos ‚Äî enfoque h√≠brido:
Streaming / real‚Äëtime: para personalizaci√≥n en la reproducci√≥n, m√©tricas en tiempo real, alertas y decisiones que requieren baja latencia (usando Mantis, Suro/Kafka, stream processors).
Batch: para feature engineering a gran escala, ETL y entrenamiento de modelos (Spark/EMR sobre S3), procesamientos peri√≥dicos y backfills.
En la pr√°ctica Netflix usa una arquitectura que combina pipelines streaming (para latencia y eventos) y pipelines batch (para computaci√≥n a gran escala y reproducibilidad), integrando ambos resultados en la infraestructura de serving y en su feature store/capas de serving.

4. **¬øQu√© puedes aprender de su arquitectura?**
   wow, todavia estoy digiriendo todo la arquitectura de netflix

---

### Pregunta 3: Tu Caso de Uso

**Piensa en un proyecto personal que te gustar√≠a hacer.**

**Describe:**
1. **¬øQu√© problema resuelve?**
   ___________________________________________

2. **¬øQu√© datos necesitas?**
   ___________________________________________

3. **¬øDe d√≥nde vienen esos datos?**
   ___________________________________________

4. **¬øC√≥mo los procesar√≠as?** (Describe tu pipeline)
   ___________________________________________

5. **¬øQu√© herramientas usar√≠as?**
   ___________________________________________

---

## Secci√≥n 9: Desaf√≠o del M√≥dulo üèÜ

### Proyecto Mini: Pipeline de Datos Meteorol√≥gicos

**Objetivo**: Crear un pipeline ETL que procese datos del clima.

**Requisitos:**

1. **Extract**: Usar API p√∫blica del clima (ejemplo: OpenWeatherMap)
2. **Transform**: 
   - Convertir temperatura de Kelvin a Celsius
   - Extraer fecha/hora
   - Calcular promedio, m√°xima, m√≠nima del d√≠a
3. **Load**: Guardar en CSV con formato limpio
4. **Bonus**: Generar gr√°fico simple de temperatura

**Estructura sugerida:**

```python
# weather_pipeline.py
import requests
import pandas as pd
from datetime import datetime

API_KEY = "tu_api_key"  # Obtener de openweathermap.org
CITY = "Madrid"

def extract_weather_data(city, api_key):
    """Llama a la API y obtiene datos"""
    # Tu c√≥digo aqu√≠
    pass

def transform_weather_data(raw_data):
    """Transforma y limpia los datos"""
    # Tu c√≥digo aqu√≠
    pass

def load_weather_data(df, filename):
    """Guarda en CSV"""
    # Tu c√≥digo aqu√≠
    pass

def run_pipeline():
    """Ejecuta el pipeline completo"""
    print("üå§Ô∏è Iniciando pipeline meteorol√≥gico...")
    # Tu c√≥digo aqu√≠
    pass

if __name__ == "__main__":
    run_pipeline()
```

**Criterios de √©xito:**
- [ ] Pipeline se ejecuta sin errores
- [ ] Datos se guardan correctamente
- [ ] Transformaciones aplicadas
- [ ] C√≥digo comentado
- [ ] Manejo b√°sico de errores

---

## Autoevaluaci√≥n

### ¬øCu√°ntas preguntas/ejercicios completaste?

- Opci√≥n m√∫ltiple (1-8): ___ / 8
- Verdadero/Falso (1-8): ___ / 8
- Relacionar conceptos: ___ / 5
- Completar c√≥digo: ___ / 2
- An√°lisis de casos: ___ / 2
- Ejercicios pr√°cticos: ___ / 3
- Investigaci√≥n: ___ / 3
- Desaf√≠o del m√≥dulo: ___ / 1

**Total**: ___ / 32

### Reflexi√≥n

**Lo que mejor entend√≠:**
___________________________________________
___________________________________________

**Lo que m√°s me cost√≥:**
___________________________________________
___________________________________________

**Preguntas que me quedaron:**
___________________________________________
___________________________________________

**Tiempo dedicado al m√≥dulo:** ___ horas

---

**Siguiente paso**: Revisa [retroalimentacion.md](retroalimentacion.md) para ver las soluciones y explicaciones, luego registra tu progreso en [progreso.md](progreso.md).

**¬°Excelente trabajo! üéâ**
