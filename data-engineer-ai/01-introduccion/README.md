# M√≥dulo 1: Introducci√≥n al Data Engineering para IA

## Introducci√≥n

¬°Bienvenido al primer m√≥dulo del curso de Data Engineering para Sistemas de IA! En este m√≥dulo estableceremos los fundamentos necesarios para comprender qu√© es el Data Engineering, por qu√© es crucial para los sistemas de IA modernos, y qu√© herramientas y t√©cnicas utilizar√°s a lo largo del curso.

## Objetivos del M√≥dulo

Al finalizar este m√≥dulo, ser√°s capaz de:

- üéØ Comprender el rol y responsabilidades de un Data Engineer
- üéØ Identificar las diferencias entre Data Engineer, Data Scientist y ML Engineer
- üéØ Conocer las arquitecturas de datos modernas (batch vs streaming, data lakes, data warehouses)
- üéØ Familiarizarte con el ecosistema de herramientas de Data Engineering
- üéØ Entender c√≥mo los datos alimentan sistemas de IA y ML
- üéØ Configurar tu entorno de desarrollo

## ¬øPor qu√© es importante?

Los sistemas de IA m√°s sofisticados del mundo (ChatGPT, recomendaciones de Netflix, b√∫squeda de Google) no funcionar√≠an sin Data Engineers que construyan y mantengan la infraestructura de datos subyacente. 

**Dato importante**: Se estima que Data Engineers pasan 80% de su tiempo en preparaci√≥n de datos y solo 20% en modelado. ¬°Los datos de calidad son el fundamento de la IA!

## Conceptos Principales

### 1. ¬øQu√© es Data Engineering?

**Data Engineering** es la disciplina de dise√±ar, construir y mantener sistemas que recolectan, almacenan, procesan y proveen datos de manera confiable, escalable y eficiente.

**Analog√≠a del mundo real:**

Imagina que eres el gerente de una gran cadena de restaurantes. Los Data Engineers son como tu equipo de log√≠stica:
- **Recolectan** ingredientes de proveedores (fuentes de datos)
- **Almacenan** en bodegas organizadas (data warehouses)
- **Procesan** y preparan los ingredientes (pipelines ETL)
- **Entregan** a las cocinas justo cuando se necesitan (APIs de datos)
- **Aseguran calidad** de los ingredientes (data quality)
- **Mantienen** la cadena de fr√≠o y frescura (data reliability)

### 2. El Rol del Data Engineer

**Responsabilidades principales:**

‚úÖ **Dise√±ar arquitecturas de datos**
- Decidir c√≥mo organizar y almacenar datos
- Elegir las tecnolog√≠as apropiadas
- Dise√±ar para escalabilidad

‚úÖ **Construir pipelines de datos**
- ETL/ELT (Extract, Transform, Load)
- Procesamiento batch y streaming
- Orquestaci√≥n de workflows

‚úÖ **Garantizar calidad de datos**
- Validaci√≥n y limpieza
- Manejo de datos faltantes
- Testing de pipelines

‚úÖ **Optimizar rendimiento**
- Queries eficientes
- Almacenamiento optimizado
- Costos controlados

‚úÖ **Colaborar con equipos**
- Data Scientists necesitan datos limpios
- ML Engineers necesitan features
- Analistas necesitan reportes

### 3. Data Engineer vs Data Scientist vs ML Engineer

| Aspecto | Data Engineer | Data Scientist | ML Engineer |
|---------|---------------|----------------|-------------|
| **Foco principal** | Infraestructura de datos | Insights y an√°lisis | Modelos en producci√≥n |
| **Habilidades clave** | ETL, SQL, Pipelines | Estad√≠stica, ML, Visualizaci√≥n | ML, MLOps, Escalabilidad |
| **Herramientas** | Airflow, Spark, SQL | Python, R, Jupyter | TensorFlow, PyTorch, MLflow |
| **Entregables** | Pipelines, APIs de datos | Reportes, modelos | Modelos en producci√≥n |
| **Lenguajes** | Python, SQL, Scala | Python, R, SQL | Python, Go |

**En proyectos peque√±os**, una persona puede hacer varios roles.  
**En empresas grandes**, cada rol es especializado.

### 4. Arquitecturas de Datos Modernas

#### A) Data Warehouse vs Data Lake vs Lakehouse

**Data Warehouse:**
- Datos **estructurados** y limpios
- Optimizado para an√°lisis y BI
- Esquema definido antes de cargar (schema-on-write)
- Ejemplos: Snowflake, Redshift, BigQuery

**Data Lake:**
- Datos **raw** en cualquier formato
- Almacenamiento masivo y barato
- Esquema al leer (schema-on-read)
- Ejemplos: S3, Azure Data Lake, HDFS

**Lakehouse:**
- Lo mejor de ambos mundos
- Datos raw + capacidades de warehouse
- Transacciones ACID sobre data lakes
- Ejemplos: Databricks Delta Lake, Apache Iceberg

#### B) Batch vs Streaming

**Procesamiento Batch:**
- Procesa grandes vol√∫menes de datos en intervalos
- Ejemplo: Reportes diarios, entrenamientos de modelos
- Herramientas: Apache Spark, dbt, Airflow

**Procesamiento Streaming:**
- Procesa datos en tiempo real
- Ejemplo: Detecci√≥n de fraude, recomendaciones live
- Herramientas: Kafka, Flink, Kinesis

### 5. El Data Engineer en Sistemas de IA

Los sistemas de IA modernos dependen de Data Engineers para:

**Para entrenamiento de modelos:**
- Recolectar y limpiar datos de entrenamiento
- Crear datasets balanceados
- Versionar datasets
- Generar features

**Para sistemas RAG (Retrieval Augmented Generation):**
- Ingestar documentos de m√∫ltiples fuentes
- Chunking y preprocesamiento de texto
- Generar embeddings
- Mantener bases de datos vectoriales actualizadas

**Para inferencia:**
- Proveer features en tiempo real
- Gestionar cach√© de predicciones
- Logging de inputs/outputs
- Monitoreo de data drift

### 6. El Ecosistema de Herramientas

**Lenguajes:**
- üêç **Python**: El lenguaje principal
- üíæ **SQL**: Esencial para datos
- ‚òï **Scala**: Para Spark (opcional)

**Orquestaci√≥n:**
- üéµ **Apache Airflow**: Orquestador m√°s popular
- üîÑ **Prefect**: Alternativa moderna
- ‚ö° **Dagster**: Enfocado en data assets

**Procesamiento:**
- ‚ö° **Apache Spark**: Procesamiento distribuido
- üêº **Pandas**: Procesamiento local
- üêª‚Äç‚ùÑÔ∏è **Polars**: Pandas pero m√°s r√°pido
- üî® **dbt**: Transformaciones SQL

**Almacenamiento:**
- üóÑÔ∏è **PostgreSQL**: Base relacional confiable
- ü™£ **S3**: Object storage en AWS
- üèîÔ∏è **Snowflake**: Data warehouse cloud
- üìä **Redshift**: Data warehouse de AWS

**IA/ML:**
- ü¶ú **LangChain**: Framework para LLMs
- ü§ó **Hugging Face**: Modelos pre-entrenados
- üìä **MLflow**: Tracking de experimentos
- üéØ **Pinecone/Chroma**: Bases vectoriales

**Cloud:**
- ‚òÅÔ∏è **AWS**: M√°s usado en empresas
- üîµ **Azure**: Fuerte en corporate
- üî¥ **GCP**: Excelente para ML

### 7. Ciclo de Vida de un Proyecto de Datos

```
1. INGESTA
   ‚Üì Recolectar datos de fuentes
   
2. ALMACENAMIENTO
   ‚Üì Guardar en data lake/warehouse
   
3. TRANSFORMACI√ìN
   ‚Üì Limpiar, normalizar, enriquecer
   
4. MODELADO (opcional)
   ‚Üì Entrenar modelos ML
   
5. SERVICIO
   ‚Üì APIs, dashboards, aplicaciones
   
6. MONITOREO
   ‚Üì Observabilidad y alertas
```

Este ciclo se repite continuamente.

## Implementaci√≥n Pr√°ctica

### Ejercicio 1: Configuraci√≥n del Entorno

**Objetivo**: Preparar tu m√°quina para el curso.

**Pasos:**

1. **Instalar Python 3.10+**
```bash
python --version  # Verificar versi√≥n
```

2. **Crear entorno virtual**
```bash
python -m venv venv-data-eng
source venv-data-eng/bin/activate  # Linux/Mac
venv-data-eng\Scripts\activate     # Windows
```

3. **Instalar dependencias b√°sicas**
```bash
pip install pandas numpy jupyter requests python-dotenv
```

4. **Instalar Docker** (para contenedores)
- Descarga de [docker.com](https://www.docker.com/)
- Verifica: `docker --version`

5. **Configurar Git**
```bash
git config --global user.name "Tu Nombre"
git config --global user.email "tu@email.com"
```

### Ejercicio 2: Tu Primer Script de Data Engineering

**Objetivo**: Crear un simple pipeline ETL.

```python
# mi_primer_pipeline.py
import pandas as pd
from datetime import datetime

def extract():
    """Extrae datos de una fuente"""
    # Simulamos datos de ventas
    data = {
        'fecha': ['2024-01-01', '2024-01-02', '2024-01-03'],
        'producto': ['Laptop', 'Mouse', 'Teclado'],
        'precio': [1200, 25, 80],
        'cantidad': [2, 10, 5]
    }
    df = pd.DataFrame(data)
    print("‚úÖ Extracci√≥n completada")
    return df

def transform(df):
    """Transforma los datos"""
    # Calcular total de venta
    df['total'] = df['precio'] * df['cantidad']
    
    # Convertir fecha a datetime
    df['fecha'] = pd.to_datetime(df['fecha'])
    
    # Agregar timestamp de procesamiento
    df['procesado_en'] = datetime.now()
    
    print("‚úÖ Transformaci√≥n completada")
    return df

def load(df, filename='ventas_procesadas.csv'):
    """Carga datos al destino"""
    df.to_csv(filename, index=False)
    print(f"‚úÖ Datos cargados en {filename}")

# Ejecutar pipeline
if __name__ == "__main__":
    print("üöÄ Iniciando pipeline ETL...")
    
    # ETL
    raw_data = extract()
    transformed_data = transform(raw_data)
    load(transformed_data)
    
    print("‚ú® Pipeline completado exitosamente!")
```

**Ejecuta:**
```bash
python mi_primer_pipeline.py
```

### Ejercicio 3: An√°lisis de un Caso Real

**Caso: Sistema de Recomendaci√≥n de Netflix**

Investiga y responde:
1. ¬øQu√© datos necesita Netflix para recomendar series?
2. ¬øDe d√≥nde vienen esos datos?
3. ¬øC√≥mo se procesan? (batch o streaming)
4. ¬øQu√© rol juega el Data Engineer?

## Mejores Pr√°cticas

### 1. Dise√±o de Datos

‚úÖ **Usa convenciones de nombres consistentes**
```python
# Bueno
user_purchase_events
transaction_timestamp

# Malo
UserPurchaseEvents
trans_ts
```

‚úÖ **Documenta tus pipelines**
```python
def process_user_data(df: pd.DataFrame) -> pd.DataFrame:
    """
    Procesa datos de usuarios.
    
    Args:
        df: DataFrame con columnas [user_id, name, email, created_at]
    
    Returns:
        DataFrame limpio y normalizado
    """
    # c√≥digo aqu√≠
```

‚úÖ **Maneja errores gracefully**
```python
try:
    df = extract_data()
except ConnectionError as e:
    logger.error(f"Error conectando a la fuente: {e}")
    # Reintentar o alertar
```

### 2. C√≥digo Limpio

‚úÖ **Funciones peque√±as y enfocadas**
```python
# Bueno: cada funci√≥n hace una cosa
def validate_email(email: str) -> bool:
    return '@' in email

def clean_email(email: str) -> str:
    return email.lower().strip()
```

‚úÖ **Usa type hints**
```python
from typing import List, Dict

def aggregate_sales(sales: List[Dict]) -> float:
    return sum(s['amount'] for s in sales)
```

### 3. Testing

‚úÖ **Testea transformaciones cr√≠ticas**
```python
import pytest

def test_calculate_total():
    data = {'precio': [10], 'cantidad': [2]}
    df = pd.DataFrame(data)
    result = transform(df)
    assert result['total'][0] == 20
```

## De Open Source a Enterprise

### Comparativa de Herramientas

| Categor√≠a | Open Source | Enterprise | ¬øCu√°ndo usar cada uno? |
|-----------|-------------|------------|------------------------|
| **Orquestaci√≥n** | Apache Airflow | AWS Step Functions, Azure Data Factory | OS: Control total, customizaci√≥n. Ent: Menos mantenimiento |
| **Data Warehouse** | PostgreSQL, ClickHouse | Snowflake, Redshift, BigQuery | OS: Presupuesto limitado. Ent: Escala y soporte |
| **Processing** | Pandas, Polars | Databricks, AWS Glue | OS: Datos peque√±os/medianos. Ent: Big data |
| **Monitoring** | Grafana, Prometheus | Datadog, New Relic | OS: Flexibilidad. Ent: Setup r√°pido |

### Transferencia de Habilidades

Las habilidades core son **transferibles**:

‚úÖ **Conceptos fundamentales**: ETL, data modeling, quality
- Si aprendes con Airflow ‚Üí Puedes usar Prefect, Dagster, Step Functions
- Si aprendes SQL en PostgreSQL ‚Üí Es el mismo en Snowflake, Redshift

‚úÖ **Arquitecturas**: Batch, streaming, lambda architecture
- Los patrones son los mismos en cualquier stack

‚úÖ **Python**: Universal en data engineering
- Funciona con cualquier herramienta cloud o open source

**Consejo**: Aprende los fundamentos con open source (gratis), luego migra a enterprise cuando trabajes en una empresa.

## Conceptos Clave para Recordar

- üîë **Data Engineering**: Construir sistemas confiables para mover y transformar datos
- üîë **Pipeline ETL**: Extract ‚Üí Transform ‚Üí Load
- üîë **Data Warehouse**: Almacena datos estructurados para an√°lisis
- üîë **Data Lake**: Almacena datos raw en cualquier formato
- üîë **Batch vs Streaming**: Procesamiento peri√≥dico vs tiempo real
- üîë **Data Quality**: La base de buenos sistemas de IA

## Pr√≥ximos Pasos

En el **M√≥dulo 2: ETL Pipelines** aprender√°s:
- Dise√±ar pipelines ETL complejos
- Usar Apache Airflow para orquestaci√≥n
- Manejar errores y reintentos
- Mejores pr√°cticas de logging

**¬øQu√© necesitas saber antes de continuar?**
‚úÖ Qu√© hace un Data Engineer  
‚úÖ Diferencia entre batch y streaming  
‚úÖ Qu√© es un pipeline ETL  
‚úÖ Python b√°sico  

## Recursos Adicionales

### Lectura
- üìñ [The Data Engineering Cookbook](https://github.com/andkret/Cookbook) - Gu√≠a completa gratuita
- üìñ "Designing Data-Intensive Applications" - Martin Kleppmann
- üìñ AWS Well-Architected Framework - Data Analytics Lens

### Videos
- üé• [What is Data Engineering?](https://www.youtube.com/watch?v=qWru-b6m030) - Seattle Data Guy
- üé• [Data Engineering Roadmap 2024](https://www.youtube.com/watch?v=iOPafVW2dAU)

### Comunidades
- üí¨ r/dataengineering (Reddit)
- üí¨ Data Engineering Discord
- üí¨ LinkedIn Data Engineering Groups

### Herramientas para Practicar
- üõ†Ô∏è [Mode SQL Tutorial](https://mode.com/sql-tutorial/) - Aprende SQL gratis
- üõ†Ô∏è [Kaggle Datasets](https://www.kaggle.com/datasets) - Datos para practicar
- üõ†Ô∏è [AWS Free Tier](https://aws.amazon.com/free/) - Practica en la nube

## Reflexi√≥n Final

> "Los datos son el nuevo petr√≥leo, pero sin refinaci√≥n (Data Engineering) no valen nada."

Has dado el primer paso en un campo con enorme demanda y oportunidades. El Data Engineering es la base invisible que hace funcionar la IA moderna.

**Tu desaf√≠o**: Piensa en un problema que te gustar√≠a resolver con datos. Podr√≠a ser:
- Analizar tus gastos personales
- Recomendar pel√≠culas basado en tu historial
- Predecir demanda de productos
- Automatizar reportes de tu trabajo

Este ser√° tu proyecto motivaci√≥n durante el curso.

---

**¬°Felicitaciones por completar el M√≥dulo 1!** üéâ

Ahora ve a [actividad-interactiva.md](actividad-interactiva.md) para poner en pr√°ctica lo aprendido.
