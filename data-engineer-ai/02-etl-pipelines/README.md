# MÃ³dulo 2: ETL Pipelines

## IntroducciÃ³n

En este mÃ³dulo profundizaremos en el diseÃ±o e implementaciÃ³n de pipelines ETL (Extract, Transform, Load) robustos y escalables. AprenderÃ¡s a orquestar workflows complejos usando Apache Airflow, la herramienta mÃ¡s popular en la industria.

## Objetivos del MÃ³dulo

Al finalizar este mÃ³dulo, serÃ¡s capaz de:

- ğŸ¯ DiseÃ±ar pipelines ETL eficientes y mantenibles
- ğŸ¯ Implementar transformaciones complejas con Pandas y SQL
- ğŸ¯ Orquestar workflows usando Apache Airflow
- ğŸ¯ Manejar errores y reintentos en pipelines
- ğŸ¯ Implementar logging y monitoring
- ğŸ¯ Aplicar mejores prÃ¡cticas de ETL

## Â¿Por quÃ© es importante?

Los pipelines ETL son el corazÃ³n de cualquier sistema de datos. Un pipeline bien diseÃ±ado significa datos confiables, procesamiento eficiente y sistemas escalables. Las empresas dependen de estos pipelines para tomar decisiones basadas en datos.

## Conceptos Principales

### 1. AnatomÃ­a de un Pipeline ETL

**Extract (ExtracciÃ³n)**
- Obtener datos de mÃºltiples fuentes
- APIs REST, bases de datos, archivos, streams
- Manejo de autenticaciÃ³n y rate limits
- ValidaciÃ³n inicial de datos

**Transform (TransformaciÃ³n)**
- Limpieza de datos
- NormalizaciÃ³n y estandarizaciÃ³n
- Enriquecimiento con datos adicionales
- Agregaciones y cÃ¡lculos
- Validaciones de calidad

**Load (Carga)**
- InserciÃ³n en destino (warehouse, database, lake)
- Estrategias: full load, incremental, upsert
- ValidaciÃ³n post-carga
- ActualizaciÃ³n de metadatos

### 2. Apache Airflow: El Orquestador

**Â¿QuÃ© es Airflow?**
- Plataforma para programar y monitorear workflows
- Define workflows como cÃ³digo (DAGs)
- Interfaz web para monitoreo
- GestiÃ³n de dependencias entre tareas
- Reintentos automÃ¡ticos y alertas

**Conceptos clave:**

```python
# DAG: Directed Acyclic Graph
# Define el flujo de trabajo

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

dag = DAG(
    'mi_pipeline',
    start_date=datetime(2024, 1, 1),
    schedule='@daily',  # Ejecuta diariamente
    catchup=False
)

# Tareas del pipeline
extract_task = PythonOperator(
    task_id='extract',
    python_callable=extract_data,
    dag=dag
)

transform_task = PythonOperator(
    task_id='transform',
    python_callable=transform_data,
    dag=dag
)

# Definir dependencias
extract_task >> transform_task  # extract debe completarse antes
```

### 3. Patrones de ETL

**Full Load (Carga Completa)**
```python
def full_load():
    # Extraer TODOS los datos cada vez
    data = extract_all_records()
    # Reemplazar tabla destino
    load_replace(data)
```
- **Pros**: Simple, estado consistente
- **Contras**: Lento para grandes volÃºmenes

**Incremental Load (Carga Incremental)**
```python
def incremental_load(last_timestamp):
    # Solo datos nuevos desde Ãºltimo run
    new_data = extract_since(last_timestamp)
    # Agregar a tabla existente
    load_append(new_data)
```
- **Pros**: Eficiente, rÃ¡pido
- **Contras**: MÃ¡s complejo, requiere tracking

**Upsert (Update + Insert)**
```python
def upsert_load(data):
    # Actualiza si existe, inserta si no
    for record in data:
        if exists(record['id']):
            update(record)
        else:
            insert(record)
```
- **Pros**: Maneja actualizaciones
- **Contras**: MÃ¡s lento que append

### 4. Transformaciones con Pandas

**Limpieza de datos:**
```python
import pandas as pd

def clean_data(df):
    # Eliminar duplicados
    df = df.drop_duplicates()
    
    # Manejar nulos
    df['price'] = df['price'].fillna(0)
    
    # Normalizar strings
    df['name'] = df['name'].str.lower().str.strip()
    
    # Convertir tipos
    df['date'] = pd.to_datetime(df['date'])
    
    return df
```

**Agregaciones:**
```python
def aggregate_sales(df):
    return df.groupby('date').agg({
        'amount': 'sum',
        'quantity': 'count',
        'price': 'mean'
    }).reset_index()
```

### 5. Manejo de Errores

**Estrategias de retry:**
```python
from airflow.operators.python import PythonOperator

task = PythonOperator(
    task_id='extract_api',
    python_callable=extract_from_api,
    retries=3,  # Reintentar 3 veces
    retry_delay=timedelta(minutes=5)
)
```

**Try-catch con logging:**
```python
import logging

def extract_data():
    try:
        data = api.fetch_data()
        logging.info(f"Extracted {len(data)} records")
        return data
    except ConnectionError as e:
        logging.error(f"Connection failed: {e}")
        raise  # Re-lanzar para que Airflow lo detecte
    except ValueError as e:
        logging.warning(f"Data issue: {e}")
        return []  # Retornar vacÃ­o pero no fallar
```

### 6. Idempotencia

**Principio**: Un pipeline debe poder ejecutarse mÃºltiples veces produciendo el mismo resultado.

```python
# Mal: No idempotente
def process_sales():
    sales = extract_all()
    db.insert(sales)  # Duplica en cada ejecuciÃ³n

# Bien: Idempotente
def process_sales(date):
    sales = extract_for_date(date)
    db.delete_for_date(date)  # Limpia primero
    db.insert(sales)  # Inserta limpio
```

## ImplementaciÃ³n PrÃ¡ctica

### Ejercicio 1: Pipeline ETL con Pandas

```python
import pandas as pd
from datetime import datetime

def extract_sales_data():
    """Simula extracciÃ³n de API"""
    return pd.DataFrame({
        'order_id': [1, 2, 3, 2],  # Duplicado
        'product': ['Laptop ', 'Mouse', None, 'mouse'],
        'price': [1000, 25, 50, None],
        'quantity': [1, 2, 1, 2],
        'date': ['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-02']
    })

def transform_sales_data(df):
    """Transforma y limpia"""
    # Eliminar duplicados
    df = df.drop_duplicates(subset=['order_id', 'date'])
    
    # Limpiar productos
    df['product'] = df['product'].fillna('Unknown')
    df['product'] = df['product'].str.lower().str.strip()
    
    # Llenar precios con promedio
    df['price'] = df['price'].fillna(df['price'].mean())
    
    # Calcular total
    df['total'] = df['price'] * df['quantity']
    
    # Convertir fecha
    df['date'] = pd.to_datetime(df['date'])
    
    # Agregar metadata
    df['processed_at'] = datetime.now()
    
    return df

def load_sales_data(df):
    """Carga a destino"""
    df.to_csv('sales_clean.csv', index=False)
    return len(df)

# Ejecutar pipeline
raw_data = extract_sales_data()
clean_data = transform_sales_data(raw_data)
rows_loaded = load_sales_data(clean_data)
print(f"Pipeline completado: {rows_loaded} registros")
```

### Ejercicio 2: Tu Primer DAG en Airflow

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

def extract(**context):
    # Extrae datos
    data = {'records': 100}
    context['task_instance'].xcom_push(key='data', value=data)
    print("âœ… Extract completado")

def transform(**context):
    # Obtiene datos de extract
    ti = context['task_instance']
    data = ti.xcom_pull(key='data', task_ids='extract')
    # Transforma
    transformed = {'records': data['records'] * 2}
    ti.xcom_push(key='transformed', value=transformed)
    print("âœ… Transform completado")

def load(**context):
    # Obtiene datos transformados
    ti = context['task_instance']
    data = ti.xcom_pull(key='transformed', task_ids='transform')
    print(f"âœ… Load completado: {data['records']} records")

# Definir DAG
default_args = {
    'owner': 'data-engineer',
    'depends_on_past': False,
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'etl_pipeline_example',
    default_args=default_args,
    description='Pipeline ETL de ejemplo',
    schedule=timedelta(days=1),
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['etl', 'example']
)

# Crear tareas
extract_task = PythonOperator(
    task_id='extract',
    python_callable=extract,
    dag=dag
)

transform_task = PythonOperator(
    task_id='transform',
    python_callable=transform,
    dag=dag
)

load_task = PythonOperator(
    task_id='load',
    python_callable=load,
    dag=dag
)

# Definir flujo
extract_task >> transform_task >> load_task
```

## Mejores PrÃ¡cticas

### 1. DiseÃ±o de Pipelines

âœ… **MantÃ©n tareas pequeÃ±as y enfocadas**
```python
# Bueno: Tareas separadas
extract_customers >> transform_customers >> load_customers
extract_orders >> transform_orders >> load_orders

# Malo: Todo en una tarea gigante
do_everything()
```

âœ… **Usa XCom para pasar datos pequeÃ±os**
```python
# Para datos pequeÃ±os (metadatos, counts)
ti.xcom_push(key='count', value=100)

# Para datos grandes, usa archivos o databases
df.to_parquet('temp_data.parquet')
```

âœ… **Implementa checks de calidad**
```python
def data_quality_check(df):
    assert len(df) > 0, "DataFrame vacÃ­o"
    assert df['price'].min() >= 0, "Precios negativos"
    assert not df['id'].duplicated().any(), "IDs duplicados"
```

### 2. ConfiguraciÃ³n

âœ… **Usa variables de entorno**
```python
import os

DB_HOST = os.getenv('DB_HOST', 'localhost')
API_KEY = os.getenv('API_KEY')
```

âœ… **Centraliza configuraciÃ³n**
```python
# config.py
class Config:
    DB_HOST = os.getenv('DB_HOST')
    DB_PORT = int(os.getenv('DB_PORT', 5432))
    BATCH_SIZE = 1000
```

### 3. Logging

âœ… **Log informaciÃ³n Ãºtil**
```python
logging.info(f"Procesando batch {batch_num}/{total_batches}")
logging.warning(f"Encontrados {null_count} valores nulos")
logging.error(f"Error en API: {error_msg}")
```

## De Open Source a Enterprise

### Comparativa de Orquestadores

| CaracterÃ­stica | Apache Airflow (OS) | AWS Step Functions | Azure Data Factory | Google Cloud Composer |
|----------------|---------------------|--------------------|--------------------|----------------------|
| **Costo** | Gratis (solo infra) | Pay per execution | Pay per activity | Pay per hora |
| **Setup** | Self-managed | Managed | Managed | Managed (Airflow) |
| **Flexibilidad** | Alta | Media | Media | Alta |
| **Curva aprendizaje** | Empinada | Suave | Media | Media |
| **IntegraciÃ³n cloud** | Manual | Nativa AWS | Nativa Azure | Nativa GCP |
| **UI** | Excelente | BÃ¡sica | Buena | Excelente |

### Transferencia de Habilidades

**De Airflow a cualquier orquestador:**

Los conceptos son universales:
- âœ… DAGs (flujo de trabajo)
- âœ… Tareas y dependencias
- âœ… Scheduling
- âœ… Reintentos y error handling
- âœ… Monitoreo y alertas

**Ejemplo de traducciÃ³n:**

**Airflow:**
```python
task1 >> task2 >> task3
```

**Step Functions:**
```json
{
  "States": {
    "Task1": {"Next": "Task2"},
    "Task2": {"Next": "Task3"}
  }
}
```

**dbt (para transformaciones SQL):**
```yaml
models:
  - name: sales_clean
    depends_on:
      - ref('sales_raw')
```

### CuÃ¡ndo usar cada uno

**Airflow (Open Source)**: 
- Control total, customizaciÃ³n
- Stack on-premise
- Budget limitado
- Equipo con experiencia en Python

**Step Functions/Data Factory (Enterprise)**:
- Ya en AWS/Azure
- Menos mantenimiento
- IntegraciÃ³n nativa
- Escalabilidad automÃ¡tica

## Conceptos Clave para Recordar

- ğŸ”‘ **ETL**: Extract, Transform, Load - proceso fundamental
- ğŸ”‘ **DAG**: Directed Acyclic Graph - define workflows
- ğŸ”‘ **Airflow**: Orquestador lÃ­der de la industria
- ğŸ”‘ **Idempotencia**: Ejecutar mÃºltiples veces = mismo resultado
- ğŸ”‘ **Incremental**: Solo procesar datos nuevos
- ğŸ”‘ **XCom**: Compartir datos entre tareas en Airflow

## PrÃ³ximos Pasos

En el **MÃ³dulo 3: Ingesta de Documentos** aprenderÃ¡s:
- Procesar PDFs, Word, HTML
- Extraer texto y tablas
- Implementar chunking strategies
- Preparar documentos para RAG

## Recursos Adicionales

### DocumentaciÃ³n
- ğŸ“– [Apache Airflow Docs](https://airflow.apache.org/docs/)
- ğŸ“– [Pandas User Guide](https://pandas.pydata.org/docs/user_guide/)
- ğŸ“– [AWS Glue ETL](https://docs.aws.amazon.com/glue/)

### Tutoriales
- ğŸ¥ [Airflow Tutorial for Beginners](https://www.youtube.com/watch?v=AHMm1wfGuHE)
- ğŸ¥ [ETL Best Practices](https://www.youtube.com/watch?v=6-oBN3zjrqY)

### Libros
- ğŸ“š "Data Pipelines Pocket Reference" - James Densmore
- ğŸ“š "Fundamentals of Data Engineering" - Joe Reis & Matt Housley

---

**Â¡Excelente trabajo completando el MÃ³dulo 2!** ğŸ‰

Ahora dominas el diseÃ±o e implementaciÃ³n de pipelines ETL. ContinÃºa a [actividad-interactiva.md](actividad-interactiva.md) para practicar.
