# M√≥dulo 2: ETL Pipelines

## Introducci√≥n

En este m√≥dulo profundizaremos en el dise√±o e implementaci√≥n de pipelines ETL (Extract, Transform, Load) robustos y escalables. Aprender√°s a orquestar workflows complejos usando Apache Airflow, la herramienta m√°s popular en la industria.

## Objetivos del M√≥dulo

Al finalizar este m√≥dulo, ser√°s capaz de:

- üéØ Dise√±ar pipelines ETL eficientes y mantenibles
- üéØ Implementar transformaciones complejas con Pandas y SQL
- üéØ Orquestar workflows usando Apache Airflow
- üéØ Manejar errores y reintentos en pipelines
- üéØ Implementar logging y monitoring
- üéØ Aplicar mejores pr√°cticas de ETL

## ¬øPor qu√© es importante?

Los pipelines ETL son el coraz√≥n de cualquier sistema de datos. Un pipeline bien dise√±ado significa datos confiables, procesamiento eficiente y sistemas escalables. Las empresas dependen de estos pipelines para tomar decisiones basadas en datos.

## Conceptos Principales

### 1. Anatom√≠a de un Pipeline ETL

**Extract (Extracci√≥n)**
- Obtener datos de m√∫ltiples fuentes
- APIs REST, bases de datos, archivos, streams
- Manejo de autenticaci√≥n y rate limits
- Validaci√≥n inicial de datos

**Transform (Transformaci√≥n)**
- Limpieza de datos
- Normalizaci√≥n y estandarizaci√≥n
- Enriquecimiento con datos adicionales
- Agregaciones y c√°lculos
- Validaciones de calidad

**Load (Carga)**
- Inserci√≥n en destino (warehouse, database, lake)
- Estrategias: full load, incremental, upsert
- Validaci√≥n post-carga
- Actualizaci√≥n de metadatos

### 2. Apache Airflow: El Orquestador

**¬øQu√© es Airflow?**
- Plataforma para programar y monitorear workflows
- Define workflows como c√≥digo (DAGs)
- Interfaz web para monitoreo
- Gesti√≥n de dependencias entre tareas
- Reintentos autom√°ticos y alertas

**Conceptos clave:**

```python
# DAG: Directed Acyclic Graph
# Define el flujo de trabajo

from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

dag = DAG(
    'mi_pipeline',
    start_date=datetime(2024, 1, 1),
    schedule='@daily',  # Ejecuta diariamente
    catchup=False
)

# Tareas del pipeline
extract_task = PythonOperator(
    task_id='extract',
    python_callable=extract_data,
    dag=dag
)

transform_task = PythonOperator(
    task_id='transform',
    python_callable=transform_data,
    dag=dag
)

# Definir dependencias
extract_task >> transform_task  # extract debe completarse antes
```

### 3. Patrones de ETL

**Full Load (Carga Completa)**
```python
def full_load():
    # Extraer TODOS los datos cada vez
    data = extract_all_records()
    # Reemplazar tabla destino
    load_replace(data)
```
- **Pros**: Simple, estado consistente
- **Contras**: Lento para grandes vol√∫menes

**Incremental Load (Carga Incremental)**
```python
def incremental_load(last_timestamp):
    # Solo datos nuevos desde √∫ltimo run
    new_data = extract_since(last_timestamp)
    # Agregar a tabla existente
    load_append(new_data)
```
- **Pros**: Eficiente, r√°pido
- **Contras**: M√°s complejo, requiere tracking

**Upsert (Update + Insert)**
```python
def upsert_load(data):
    # Actualiza si existe, inserta si no
    for record in data:
        if exists(record['id']):
            update(record)
        else:
            insert(record)
```
- **Pros**: Maneja actualizaciones
- **Contras**: M√°s lento que append

### 4. Transformaciones con Pandas

**Limpieza de datos:**
```python
import pandas as pd

def clean_data(df):
    # Eliminar duplicados
    df = df.drop_duplicates()
    
    # Manejar nulos
    df['price'] = df['price'].fillna(0)
    
    # Normalizar strings
    df['name'] = df['name'].str.lower().str.strip()
    
    # Convertir tipos
    df['date'] = pd.to_datetime(df['date'])
    
    return df
```

**Agregaciones:**
```python
def aggregate_sales(df):
    return df.groupby('date').agg({
        'amount': 'sum',
        'quantity': 'count',
        'price': 'mean'
    }).reset_index()
```

### 5. Manejo de Errores

**Estrategias de retry:**
```python
from airflow.operators.python import PythonOperator

task = PythonOperator(
    task_id='extract_api',
    python_callable=extract_from_api,
    retries=3,  # Reintentar 3 veces
    retry_delay=timedelta(minutes=5)
)
```

**Try-catch con logging:**
```python
import logging

def extract_data():
    try:
        data = api.fetch_data()
        logging.info(f"Extracted {len(data)} records")
        return data
    except ConnectionError as e:
        logging.error(f"Connection failed: {e}")
        raise  # Re-lanzar para que Airflow lo detecte
    except ValueError as e:
        logging.warning(f"Data issue: {e}")
        return []  # Retornar vac√≠o pero no fallar
```

### 6. Idempotencia

**Principio**: Un pipeline debe poder ejecutarse m√∫ltiples veces produciendo el mismo resultado.

```python
# Mal: No idempotente
def process_sales():
    sales = extract_all()
    db.insert(sales)  # Duplica en cada ejecuci√≥n

# Bien: Idempotente
def process_sales(date):
    sales = extract_for_date(date)
    db.delete_for_date(date)  # Limpia primero
    db.insert(sales)  # Inserta limpio
```

## Implementaci√≥n Pr√°ctica

### Ejercicio 1: Pipeline ETL con Pandas

```python
import pandas as pd
from datetime import datetime

def extract_sales_data():
    """Simula extracci√≥n de API"""
    return pd.DataFrame({
        'order_id': [1, 2, 3, 2],  # Duplicado
        'product': ['Laptop ', 'Mouse', None, 'mouse'],
        'price': [1000, 25, 50, None],
        'quantity': [1, 2, 1, 2],
        'date': ['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-02']
    })

def transform_sales_data(df):
    """Transforma y limpia"""
    # Eliminar duplicados
    df = df.drop_duplicates(subset=['order_id', 'date'])
    
    # Limpiar productos
    df['product'] = df['product'].fillna('Unknown')
    df['product'] = df['product'].str.lower().str.strip()
    
    # Llenar precios con promedio
    df['price'] = df['price'].fillna(df['price'].mean())
    
    # Calcular total
    df['total'] = df['price'] * df['quantity']
    
    # Convertir fecha
    df['date'] = pd.to_datetime(df['date'])
    
    # Agregar metadata
    df['processed_at'] = datetime.now()
    
    return df

def load_sales_data(df):
    """Carga a destino"""
    df.to_csv('sales_clean.csv', index=False)
    return len(df)

# Ejecutar pipeline
raw_data = extract_sales_data()
clean_data = transform_sales_data(raw_data)
rows_loaded = load_sales_data(clean_data)
print(f"Pipeline completado: {rows_loaded} registros")
```

### Ejercicio 2: Tu Primer DAG en Airflow

```python
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

def extract(**context):
    # Extrae datos
    data = {'records': 100}
    context['task_instance'].xcom_push(key='data', value=data)
    print("‚úÖ Extract completado")

def transform(**context):
    # Obtiene datos de extract
    ti = context['task_instance']
    data = ti.xcom_pull(key='data', task_ids='extract')
    # Transforma
    transformed = {'records': data['records'] * 2}
    ti.xcom_push(key='transformed', value=transformed)
    print("‚úÖ Transform completado")

def load(**context):
    # Obtiene datos transformados
    ti = context['task_instance']
    data = ti.xcom_pull(key='transformed', task_ids='transform')
    print(f"‚úÖ Load completado: {data['records']} records")

# Definir DAG
default_args = {
    'owner': 'data-engineer',
    'depends_on_past': False,
    'email_on_failure': True,
    'email_on_retry': False,
    'retries': 2,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'etl_pipeline_example',
    default_args=default_args,
    description='Pipeline ETL de ejemplo',
    schedule=timedelta(days=1),
    start_date=datetime(2024, 1, 1),
    catchup=False,
    tags=['etl', 'example']
)

# Crear tareas
extract_task = PythonOperator(
    task_id='extract',
    python_callable=extract,
    dag=dag
)

transform_task = PythonOperator(
    task_id='transform',
    python_callable=transform,
    dag=dag
)

load_task = PythonOperator(
    task_id='load',
    python_callable=load,
    dag=dag
)

# Definir flujo
extract_task >> transform_task >> load_task
```

## Mejores Pr√°cticas

### 1. Dise√±o de Pipelines

‚úÖ **Mant√©n tareas peque√±as y enfocadas**
```python
# Bueno: Tareas separadas
extract_customers >> transform_customers >> load_customers
extract_orders >> transform_orders >> load_orders

# Malo: Todo en una tarea gigante
do_everything()
```

‚úÖ **Usa XCom para pasar datos peque√±os**
```python
# Para datos peque√±os (metadatos, counts)
ti.xcom_push(key='count', value=100)

# Para datos grandes, usa archivos o databases
df.to_parquet('temp_data.parquet')
```

‚úÖ **Implementa checks de calidad**
```python
def data_quality_check(df):
    assert len(df) > 0, "DataFrame vac√≠o"
    assert df['price'].min() >= 0, "Precios negativos"
    assert not df['id'].duplicated().any(), "IDs duplicados"
```

### 2. Configuraci√≥n

‚úÖ **Usa variables de entorno**
```python
import os

DB_HOST = os.getenv('DB_HOST', 'localhost')
API_KEY = os.getenv('API_KEY')
```

‚úÖ **Centraliza configuraci√≥n**
```python
# config.py
class Config:
    DB_HOST = os.getenv('DB_HOST')
    DB_PORT = int(os.getenv('DB_PORT', 5432))
    BATCH_SIZE = 1000
```

### 3. Logging

‚úÖ **Log informaci√≥n √∫til**
```python
logging.info(f"Procesando batch {batch_num}/{total_batches}")
logging.warning(f"Encontrados {null_count} valores nulos")
logging.error(f"Error en API: {error_msg}")
```

## De Open Source a Enterprise

### Comparativa de Orquestadores

| Caracter√≠stica | Apache Airflow (OS) | AWS Step Functions | Azure Data Factory | Google Cloud Composer |
|----------------|---------------------|--------------------|--------------------|----------------------|
| **Costo** | Gratis (solo infra) | Pay per execution | Pay per activity | Pay per hora |
| **Setup** | Self-managed | Managed | Managed | Managed (Airflow) |
| **Flexibilidad** | Alta | Media | Media | Alta |
| **Curva aprendizaje** | Empinada | Suave | Media | Media |
| **Integraci√≥n cloud** | Manual | Nativa AWS | Nativa Azure | Nativa GCP |
| **UI** | Excelente | B√°sica | Buena | Excelente |

### Transferencia de Habilidades

**De Airflow a cualquier orquestador:**

Los conceptos son universales:
- ‚úÖ DAGs (flujo de trabajo)
- ‚úÖ Tareas y dependencias
- ‚úÖ Scheduling
- ‚úÖ Reintentos y error handling
- ‚úÖ Monitoreo y alertas

**Ejemplo de traducci√≥n:**

**Airflow:**
```python
task1 >> task2 >> task3
```

**Step Functions:**
```json
{
  "States": {
    "Task1": {"Next": "Task2"},
    "Task2": {"Next": "Task3"}
  }
}
```

**dbt (para transformaciones SQL):**
```yaml
models:
  - name: sales_clean
    depends_on:
      - ref('sales_raw')
```

### Cu√°ndo usar cada uno

**Airflow (Open Source)**: 
- Control total, customizaci√≥n
- Stack on-premise
- Budget limitado
- Equipo con experiencia en Python

**Step Functions/Data Factory (Enterprise)**:
- Ya en AWS/Azure
- Menos mantenimiento
- Integraci√≥n nativa
- Escalabilidad autom√°tica

## Conceptos Clave para Recordar

- üîë **ETL**: Extract, Transform, Load - proceso fundamental
- üîë **DAG**: Directed Acyclic Graph - define workflows
- üîë **Airflow**: Orquestador l√≠der de la industria
- üîë **Idempotencia**: Ejecutar m√∫ltiples veces = mismo resultado
- üîë **Incremental**: Solo procesar datos nuevos
- üîë **XCom**: Compartir datos entre tareas en Airflow

## Pr√≥ximos Pasos

En el **M√≥dulo 3: Ingesta de Documentos** aprender√°s:
- Procesar PDFs, Word, HTML
- Extraer texto y tablas
- Implementar chunking strategies
- Preparar documentos para RAG

## Recursos Adicionales

### Documentaci√≥n
- üìñ [Apache Airflow Docs](https://airflow.apache.org/docs/)
- üìñ [Pandas User Guide](https://pandas.pydata.org/docs/user_guide/)
- üìñ [AWS Glue ETL](https://docs.aws.amazon.com/glue/)

### Tutoriales
- üé• [Airflow Tutorial for Beginners](https://www.youtube.com/watch?v=AHMm1wfGuHE)
- üé• [ETL Best Practices](https://www.youtube.com/watch?v=6-oBN3zjrqY)

### Libros
- üìö "Data Pipelines Pocket Reference" - James Densmore
- üìö "Fundamentals of Data Engineering" - Joe Reis & Matt Housley

---

**¬°Excelente trabajo completando el M√≥dulo 2!** üéâ

Ahora dominas el dise√±o e implementaci√≥n de pipelines ETL. Contin√∫a a [actividad-interactiva.md](actividad-interactiva.md) para practicar.
