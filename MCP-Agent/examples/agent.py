#!/usr/bin/env python3
"""
MCP Agent - Minimal Example Implementation

This is a runnable example of an MCP Agent that processes prompts and returns responses.
Currently uses a mock LLM for demonstration purposes.

Instructions to run:
1. Make sure you have Python 3.9+ installed
2. Run directly: python examples/agent.py
3. Or use the provided run.sh script: ./run.sh

Author: MCP Agent Team
Version: 0.1-mock
"""

import json
import time
from datetime import datetime, timezone
from typing import Dict, Any, Optional


class MockLLM:
    """
    Mock LLM that simulates a language model response.
    In production, this would be replaced with actual LLM API calls (OpenAI, Anthropic, etc.)
    """
    
    def __init__(self, model_name: str = "mock-llm-v1"):
        self.model_name = model_name
        self.response_templates = {
            "default": "This is a mock response to your prompt: '{prompt}'. In a real implementation, this would be generated by an actual LLM.",
            "greeting": "Hello! I'm a mock MCP Agent. How can I help you today?",
            "python": "Python is a high-level, interpreted programming language known for its simplicity and readability.",
            "mcp": "The Model Context Protocol (MCP) is a standardized protocol for communication between applications and language models.",
        }
    
    def generate(self, prompt: str, max_tokens: int = 150, temperature: float = 0.7) -> str:
        """
        Simulate LLM response generation.
        In production: replace with actual API call to OpenAI, Anthropic, etc.
        """
        # Simulate processing time
        time.sleep(0.05)
        
        # Simple keyword matching for demo purposes
        prompt_lower = prompt.lower()
        
        if any(word in prompt_lower for word in ["hello", "hi", "hola"]):
            return self.response_templates["greeting"]
        elif "python" in prompt_lower:
            return self.response_templates["python"]
        elif "mcp" in prompt_lower or "model context protocol" in prompt_lower:
            return self.response_templates["mcp"]
        else:
            return self.response_templates["default"].format(prompt=prompt)


class MCPAgent:
    """
    MCP Agent - Main class that processes prompts and generates responses.
    
    This agent follows the MCP specification for prompt processing:
    1. Validate input
    2. Enrich with context
    3. Invoke LLM (mock in this version)
    4. Post-process response
    5. Format and return
    """
    
    def __init__(self, llm: Optional[MockLLM] = None):
        self.llm = llm or MockLLM()
        self.version = "0.1-mock"
    
    def validate_prompt(self, prompt_data: Dict[str, Any]) -> tuple[bool, Optional[str]]:
        """
        Validate the input prompt according to MCP specification.
        
        Args:
            prompt_data: Dictionary containing prompt and optional parameters
            
        Returns:
            Tuple of (is_valid, error_message)
        """
        # Check if prompt field exists
        if "prompt" not in prompt_data:
            return False, "Missing required field: 'prompt'"
        
        # Check if prompt is not empty
        if not prompt_data["prompt"] or not isinstance(prompt_data["prompt"], str):
            return False, "Field 'prompt' must be a non-empty string"
        
        # Validate optional fields if present
        if "max_tokens" in prompt_data:
            if not isinstance(prompt_data["max_tokens"], int) or prompt_data["max_tokens"] <= 0:
                return False, "Field 'max_tokens' must be a positive integer"
        
        if "temperature" in prompt_data:
            if not isinstance(prompt_data["temperature"], (int, float)):
                return False, "Field 'temperature' must be a number"
            if not 0 <= prompt_data["temperature"] <= 2:
                return False, "Field 'temperature' must be between 0 and 2"
        
        return True, None
    
    def enrich_context(self, prompt_data: Dict[str, Any]) -> str:
        """
        Enrich the prompt with additional context if provided.
        
        Args:
            prompt_data: Dictionary containing prompt and optional context
            
        Returns:
            Enriched prompt string
        """
        prompt = prompt_data["prompt"]
        
        # Add context if available
        if "context" in prompt_data and prompt_data["context"]:
            context = prompt_data["context"]
            context_str = "\n".join([f"{k}: {v}" for k, v in context.items()])
            prompt = f"Context:\n{context_str}\n\nPrompt: {prompt}"
        
        # Add role-based system message if needed
        role = prompt_data.get("role", "user")
        if role == "system":
            prompt = f"[SYSTEM] {prompt}"
        
        return prompt
    
    def process_prompt(self, prompt_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        Main processing function - orchestrates the entire prompt processing flow.
        
        Args:
            prompt_data: Dictionary containing prompt and optional parameters
            
        Returns:
            Dictionary with response and metadata according to MCP specification
        """
        start_time = time.time()
        
        # Step 1: Validate input
        is_valid, error_msg = self.validate_prompt(prompt_data)
        if not is_valid:
            return {
                "response": "",
                "status": "error",
                "error": error_msg,
                "metadata": {
                    "model": self.llm.model_name,
                    "tokens_used": 0,
                    "processing_time": time.time() - start_time,
                    "timestamp": datetime.now(timezone.utc).isoformat()
                }
            }
        
        # Step 2: Enrich with context
        enriched_prompt = self.enrich_context(prompt_data)
        
        # Step 3: Invoke LLM (mock)
        max_tokens = prompt_data.get("max_tokens", 150)
        temperature = prompt_data.get("temperature", 0.7)
        
        try:
            response_text = self.llm.generate(enriched_prompt, max_tokens, temperature)
            tokens_used = len(response_text.split())  # Simple word count as token approximation
            
            # Step 4: Post-process response
            response_text = response_text.strip()
            
            # Step 5: Format and return
            return {
                "response": response_text,
                "status": "success",
                "metadata": {
                    "model": self.llm.model_name,
                    "tokens_used": tokens_used,
                    "processing_time": round(time.time() - start_time, 3),
                    "timestamp": datetime.now(timezone.utc).isoformat()
                }
            }
            
        except Exception as e:
            return {
                "response": "",
                "status": "error",
                "error": f"Error processing prompt: {str(e)}",
                "metadata": {
                    "model": self.llm.model_name,
                    "tokens_used": 0,
                    "processing_time": round(time.time() - start_time, 3),
                    "timestamp": datetime.now(timezone.utc).isoformat()
                }
            }


def main():
    """
    Main function - demonstrates the MCP Agent with example prompts.
    """
    print("=" * 60)
    print("MCP Agent - Example Execution")
    print("=" * 60)
    print()
    
    # Initialize the agent
    agent = MCPAgent()
    
    # Example 1: Simple prompt
    print("Example 1: Simple Prompt")
    print("-" * 60)
    prompt1 = {
        "prompt": "Hello, how are you?",
        "role": "user"
    }
    print(f"Input: {json.dumps(prompt1, indent=2)}")
    result1 = agent.process_prompt(prompt1)
    print(f"Output: {json.dumps(result1, indent=2)}")
    print()
    
    # Example 2: Prompt about Python
    print("Example 2: Question about Python")
    print("-" * 60)
    prompt2 = {
        "prompt": "What is Python?",
        "role": "user",
        "max_tokens": 100
    }
    print(f"Input: {json.dumps(prompt2, indent=2)}")
    result2 = agent.process_prompt(prompt2)
    print(f"Output: {json.dumps(result2, indent=2)}")
    print()
    
    # Example 3: Prompt with context
    print("Example 3: Prompt with Context")
    print("-" * 60)
    prompt3 = {
        "prompt": "Can you elaborate on that?",
        "role": "user",
        "context": {
            "previous_question": "What is Python?",
            "previous_answer": result2["response"]
        }
    }
    print(f"Input: {json.dumps(prompt3, indent=2)}")
    result3 = agent.process_prompt(prompt3)
    print(f"Output: {json.dumps(result3, indent=2)}")
    print()
    
    # Example 4: Invalid prompt (error handling)
    print("Example 4: Invalid Prompt (Error Handling)")
    print("-" * 60)
    prompt4 = {
        "role": "user"
        # Missing 'prompt' field
    }
    print(f"Input: {json.dumps(prompt4, indent=2)}")
    result4 = agent.process_prompt(prompt4)
    print(f"Output: {json.dumps(result4, indent=2)}")
    print()
    
    print("=" * 60)
    print("âœ“ All examples completed successfully!")
    print("=" * 60)


if __name__ == "__main__":
    main()
