# ðŸŽ¯ MÃ³dulo 01 â€” IntroducciÃ³n al Pareto 20/80

## ðŸŽ¯ Objetivos de Aprendizaje

Al completar este mÃ³dulo serÃ¡s capaz de:

1. **Comprender el principio de Pareto 80/20** aplicado a Data Engineering
2. **Identificar el 20% de skills que generan 80% de impacto** en el trabajo real
3. **Planificar tu roadmap de aprendizaje** enfocado en proyectos
4. **Aplicar la metodologÃ­a projects-first** en tu estudio
5. **Establecer un plan personalizado** basado en tu tiempo disponible

---

## ðŸ“š Contenido TeÃ³rico

### 1. El Principio de Pareto en Data Engineering

**DefiniciÃ³n:**
El **Principio de Pareto** (regla 80/20) establece que aproximadamente el **80% de los resultados** provienen del **20% de las causas**.

**Aplicado a Data Engineering:**
- **20% de los skills** â†’ **80% del valor** que entregas en el trabajo
- **20% de las herramientas** â†’ **80% de los pipelines** que construirÃ¡s
- **20% de los patrones** â†’ **80% de los casos de uso** que enfrentarÃ¡s

### 2. El 20% CrÃ­tico en Data Engineering

#### 2.1 Lenguajes y Queries (20% Core)

**SQL (el skill mÃ¡s importante):**
```
âœ… JOINs (INNER, LEFT, FULL OUTER)
âœ… Window functions (ROW_NUMBER, RANK, LAG, LEAD)
âœ… GROUP BY con agregaciones
âœ… CTEs (WITH clauses)
âœ… MERGE/UPSERT statements

âŒ No necesitas (al inicio):
- SQL Server CLR functions
- Advanced PL/SQL procedures
- Database administration tasks
```

**Python para ETL:**
```
âœ… pandas (read_csv, to_sql, merge, groupby)
âœ… requests (API calls)
âœ… psycopg2/sqlalchemy (DB connections)
âœ… Error handling (try/except)

âŒ No necesitas (al inicio):
- Machine learning libraries
- Web frameworks (Flask/Django)
- Advanced decorators/metaclasses
```

**Scala para Spark:**
```
âœ… DataFrame API (select, filter, join, groupBy)
âœ… partitionBy para escritura eficiente
âœ… Functions (col, lit, when, udf)

âŒ No necesitas (al inicio):
- RDD API (legacy)
- Advanced functional programming
- Scala macros
```

#### 2.2 Patrones de ETL (20% Core)

**Cargas incrementales idempotentes:**
```sql
-- PatrÃ³n MERGE (UPSERT) - usado en 80% de pipelines
MERGE INTO target_table AS t
USING source_table AS s
ON t.id = s.id
WHEN MATCHED THEN
  UPDATE SET t.value = s.value, t.updated_at = CURRENT_TIMESTAMP
WHEN NOT MATCHED THEN
  INSERT (id, value, created_at) VALUES (s.id, s.value, CURRENT_TIMESTAMP);
```

**Particionado por fecha:**
```scala
// PatrÃ³n usado en 80% de data lakes
df.write
  .mode("overwrite")
  .partitionBy("date_partition")  // Clave para performance
  .parquet("s3://bucket/table/")
```

**DeduplicaciÃ³n:**
```sql
-- PatrÃ³n ROW_NUMBER - elimina duplicados
WITH ranked AS (
  SELECT *, ROW_NUMBER() OVER (PARTITION BY id ORDER BY updated_at DESC) AS rn
  FROM raw_data
)
SELECT * EXCLUDE rn FROM ranked WHERE rn = 1;
```

#### 2.3 OrquestaciÃ³n (20% Core)

**DiseÃ±o de DAGs en Airflow:**
```python
# PatrÃ³n bÃ¡sico usado en 80% de workflows
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime, timedelta

default_args = {
    'owner': 'dataeng',
    'retries': 3,
    'retry_delay': timedelta(minutes=5)
}

with DAG(
    'etl_pipeline',
    default_args=default_args,
    schedule_interval='0 2 * * *',  # Daily at 2 AM
    start_date=datetime(2024, 1, 1),
    catchup=False
) as dag:
    
    extract = PythonOperator(task_id='extract', python_callable=extract_func)
    transform = PythonOperator(task_id='transform', python_callable=transform_func)
    load = PythonOperator(task_id='load', python_callable=load_func)
    
    extract >> transform >> load
```

#### 2.4 Testing (20% Core)

**Checks esenciales que previenen 80% de problemas:**
```python
# 1. Row count
assert df.count() > 0, "Empty DataFrame"

# 2. Uniqueness
assert df.select('id').distinct().count() == df.count(), "Duplicate IDs"

# 3. Not NULL
assert df.filter(col('id').isNull()).count() == 0, "NULL IDs found"

# 4. Expected range
assert df.filter(col('amount') < 0).count() == 0, "Negative amounts"
```

---

### 3. MetodologÃ­a Projects-First

**Tradicional (Theory-First):**
```
1. Leer 200 pÃ¡ginas de documentaciÃ³n
2. Ver 10 horas de videos
3. Hacer ejercicios de sintaxis
4. (Tal vez) construir un proyecto
```

**Pareto 20/80 (Projects-First):**
```
1. Proyecto mini definido (ej: "ETL de CSV a PostgreSQL")
2. Aprender SOLO lo necesario para ese proyecto
3. Construir, romper, arreglar, iterar
4. Documentar aprendizajes
5. Repetir con proyecto similar (reinforcement)
```

**Ejemplo concreto:**
```
Proyecto: "Pipeline de ventas diarias"

Paso 1: Define entregables
- Input: ventas.csv (10K filas)
- Output: PostgreSQL tabla fact_ventas
- TransformaciÃ³n: agregaciÃ³n por dÃ­a y categorÃ­a
- OrquestaciÃ³n: Airflow DAG

Paso 2: Aprende SOLO lo necesario
- pandas.read_csv()
- pandas.groupby()
- sqlalchemy.create_engine()
- df.to_sql()
- Airflow BashOperator bÃ¡sico

Paso 3: Construye versiÃ³n 1 (2 horas)
Paso 4: Mejora con deduplicaciÃ³n (1 hora)
Paso 5: AÃ±ade tests (30 min)
Paso 6: Documenta (30 min)

Total: 4 horas â†’ Pipeline funcional en portfolio
```

---

### 4. Pareto 20/80 por MÃ³dulo del Curso

| MÃ³dulo | 20% to Learn (Core Skills) | 80% to Practice (Projects) |
|--------|---------------------------|----------------------------|
| **SQL** | JOINs, window functions, MERGE | Reportes de ventas, queries analÃ­ticas |
| **Python ETL** | pandas, requests, psycopg2 | ETL CSVâ†’DB, APIâ†’DB |
| **Spark Scala** | DataFrame API, partitionBy | Limpieza de datos, particionado |
| **Databricks** | Notebooks, Jobs, Delta Lake | Job productivo end-to-end |
| **Airflow** | DAGs, retries, scheduling | Orquestar pipeline completo |
| **Testing** | Row count, uniqueness, NULL checks | Suite de tests para pipeline |

---

### 5. Roadmap de Carrera Data Engineer

**Junior Data Engineer (0-2 aÃ±os):**
```
Core skills:
âœ… SQL avanzado
âœ… Python para ETL
âœ… Git/GitHub bÃ¡sico
âœ… Docker bÃ¡sico
âœ… Airflow (DAGs bÃ¡sicos)

Portfolio:
- 2-3 pipelines ETL documentados en GitHub
- Al menos 1 proyecto con Airflow
```

**Mid Data Engineer (2-4 aÃ±os):**
```
Core skills:
âœ… Spark (Scala o PySpark)
âœ… Cloud platform (AWS/Azure/GCP)
âœ… Delta Lake / Data Lakehouse
âœ… dbt para transformaciones
âœ… Testing & data quality

Portfolio:
- Pipeline end-to-end en producciÃ³n (cloud)
- Contribuciones a proyectos open source
- Blog tÃ©cnico con aprendizajes
```

**Senior Data Engineer (4+ aÃ±os):**
```
Core skills:
âœ… Arquitectura de sistemas distribuidos
âœ… OptimizaciÃ³n de costos cloud
âœ… Mentoring & code reviews
âœ… DiseÃ±o de data platforms
âœ… CI/CD para data pipelines

Logros:
- Liderazgo de proyectos de data
- DiseÃ±o de arquitecturas escalables
- Mejoras de performance (ej: 50% reducciÃ³n costos)
```

---

### 6. CÃ³mo Estudiar Este Curso (Estrategia Pareto)

**âŒ NO hagas esto:**
- Leer todos los mÃ³dulos linealmente sin practicar
- Ver videos sin ejecutar cÃ³digo
- Copiar/pegar cÃ³digo sin entenderlo
- Saltarte los proyectos ("lo harÃ© despuÃ©s")

**âœ… SÃ haz esto:**
- **1 proyecto por mÃ³dulo** (mÃ­nimo)
- **Ejecutar cada comando** en `actividad-interactiva.md`
- **Romper cÃ³digo intencionalmente** (aprender debuggeando)
- **Repetir patrones** en 3 datasets diferentes
- **Documentar aprendizajes** en tu rama personal

**Ritmo recomendado (Pareto-optimized):**
```
Semana 1-2: Setup + SQL Core + Proyecto SQL
Semana 3-4: Python ETL + Spark basics + Proyectos
Semana 5-6: Databricks + Delta Lake + Proyectos
Semana 7-8: Airflow + Testing + Proyecto integrador
Semana 9-10: Observability + Security + Proyecto final
Semana 11-12: Pulir proyecto final + Portfolio
```

---

## ðŸ‹ï¸ Actividades PrÃ¡cticas

### Actividad 1: Identificar tu 20% Personal

Revisa job postings reales de Data Engineer y lista las skills mÃ¡s mencionadas.

### Actividad 2: Crear tu Roadmap Personal

Basado en tu tiempo disponible, planifica quÃ© mÃ³dulos completarÃ¡s y en quÃ© orden.

### Actividad 3: Configurar Sistema de Tracking

Crea un documento personal de progreso (puede ser en `progreso.md`).

---

## ðŸ“ Entregables

Al finalizar este mÃ³dulo:

1. âœ… Documento con tu 20% personal identificado
2. âœ… Roadmap personalizado (timeline)
3. âœ… Sistema de tracking configurado
4. âœ… Primer mini-proyecto definido

---

## ðŸŽ¯ Criterios de Ã‰xito

- [ ] Comprendes el principio Pareto 80/20
- [ ] Identificaste el 20% crÃ­tico en Data Engineering
- [ ] Tienes un plan de estudio personalizado
- [ ] Entiendes la metodologÃ­a projects-first
- [ ] EstÃ¡s motivado para comenzar con proyectos

---

## â±ï¸ DuraciÃ³n Estimada

- **Lectura de teorÃ­a:** 45 minutos
- **Actividades prÃ¡cticas:** 1 hora
- **PlanificaciÃ³n personal:** 30 minutos

**Total: 2-2.5 horas**

---

## ðŸ“š Recursos Adicionales

Ver `recursos.md` para:
- Job postings reales analizados
- Roadmaps de otros Data Engineers
- ArtÃ­culos sobre Pareto en tech skills

---

## â­ï¸ Siguiente Paso

**MÃ³dulo 02: SQL Core** â€” Donde aplicarÃ¡s el principio Pareto para dominar el 20% de SQL que usarÃ¡s en el 80% de tus pipelines.

---

**ðŸ’¡ Tip:** El principio Pareto no significa ignorar el 80% restante. Significa **priorizar inteligentemente** para maximizar ROI de tu tiempo de estudio.
