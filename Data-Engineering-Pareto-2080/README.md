# ğŸ¯ Data Engineering Pareto 20/80 â€” Projects-First Course

> **Aprende el 20% de Data Engineering que genera el 80% del impacto profesional**

Este curso prÃ¡ctico sigue el **principio de Pareto**: identifica el **20% de conocimientos crÃ­ticos** que generan el **80% del valor** en el trabajo real de Data Engineering. Prioriza proyectos verificables sobre teorÃ­a abstracta.

---

## ğŸš€ Quick Start

```bash
# 1. Clonar repositorio
git clone https://github.com/angra8410/all-my-learnings.git
cd all-my-learnings/Data-Engineering-Pareto-2080

# 2. Leer el plan de estudio
cat plan-estudio-pareto-data-engineering.md

# 3. Crear rama personal
git checkout -b feature/mi-progreso-pareto

# 4. Comenzar con MÃ³dulo 00
cd 00-plan-setup
cat README.md
```

---

## ğŸ“š Estructura del Curso

### Plan de Estudio
ğŸ“„ **[plan-estudio-pareto-data-engineering.md](plan-estudio-pareto-data-engineering.md)**  
Resumen ejecutivo del curso con objetivos, metodologÃ­a Pareto, estructura de mÃ³dulos y acceptance criteria.

### 13 MÃ³dulos (120+ horas)

| # | MÃ³dulo | DuraciÃ³n | Proyecto | Status |
|---|--------|----------|----------|--------|
| **00** | [Plan & Setup](00-plan-setup/) | 3-4h | Setup completo | âœ… |
| **01** | [Intro Pareto 20/80](01-intro-pareto/) | 2h | Roadmap personal | âœ… |
| **02** | [SQL Core](02-sql-core/) | 10-12h | Reporting queries | ğŸ“ |
| **03** | [Python ETL Basics](03-python-etl-basics/) | 8-10h | ETL pipeline | ğŸ“ |
| **04** | [Spark Scala Fundamentals](04-spark-scala-fundamentals/) | 12-15h | Data cleaning job | ğŸ“ |
| **05** | [Databricks Workflow](05-databricks-workflow/) | 8-10h | Production job | ğŸ“ |
| **06** | [Delta Lake & Storage](06-delta-lake-storage/) | 8-10h | SCD Type 2 | ğŸ“ |
| **07** | [DBT or Transforms](07-dbt-transforms/) | 10-12h | Data mart | ğŸ“ |
| **08** | [Airflow Orchestration](08-airflow-orchestration/) | 12-15h | DAG completo | ğŸ“ |
| **09** | [Testing & Data Quality](09-testing-data-quality/) | 8-10h | Test suite | ğŸ“ |
| **10** | [Observability & Cost](10-observability-cost/) | 6-8h | Monitoring dashboard | ğŸ“ |
| **11** | [Security & Governance](11-security-governance/) | 4-6h | Secure pipeline | ğŸ“ |
| **12** | [Final Project](12-final-project/) | 20-25h | End-to-end integrator | ğŸ“ |

**Leyenda:**  
âœ… = Contenido completo  
ğŸ“ = Estructura creada (placeholder files)

### Cada MÃ³dulo Incluye

Formato estandarizado de 5 archivos:

1. **README.md**: Objetivos, teorÃ­a, actividades prÃ¡cticas
2. **actividad-interactiva.md**: Ejercicios con comandos verificables, campos para respuestas, duraciones
3. **progreso.md**: Checklist de avance personal
4. **retroalimentacion.md**: RÃºbrica de evaluaciÃ³n con porcentajes
5. **recursos.md**: Datasets, links, cÃ³digo de ejemplo

---

## ğŸ› ï¸ Helper Artifacts

**[helper-artifacts/](helper-artifacts/)** â€” Code snippets reutilizables del 20% core:

### 1. Airflow DAG Skeleton
ğŸ“„ `airflow/minimal_dag_skeleton.py` (10KB)
- Template completo con best practices
- Retries, scheduling, XCom
- Comentarios explicativos

### 2. Spark Scala Job
ğŸ“„ `spark-scala/DataCleaningJob.scala` (12KB)
- Job completo de limpieza de datos
- Transformaciones core
- Escritura particionada
- Data quality checks integrados

### 3. SQL DDL Examples
ğŸ“„ `sql/ddl_fact_dimension_tables.sql` (16KB)
- Star schema completo
- Fact y dimension tables
- SCD Type 1 y Type 2
- Indexes, constraints, partitioning

### 4. SQL Data Quality Tests
ğŸ“„ `sql/data_quality_tests.sql` (15KB)
- Tests de completeness, uniqueness, validity
- Statistical outlier detection
- Test suite automatizado (PL/pgSQL)

---

## ğŸ¯ Principio Pareto Aplicado

### 20% de Skills (High-Impact Core)

**SQL:**
```sql
-- JOINs, window functions, MERGE/UPSERT
WITH ranked AS (
  SELECT *, ROW_NUMBER() OVER (PARTITION BY id ORDER BY date DESC) AS rn
  FROM data
)
SELECT * FROM ranked WHERE rn = 1;
```

**Spark/Scala:**
```scala
// DataFrame API, particionado
df.write
  .mode("overwrite")
  .partitionBy("date_partition")
  .parquet("s3://bucket/table/")
```

**Airflow:**
```python
# DAG design con retries
default_args = {
    'retries': 3,
    'retry_delay': timedelta(minutes=5)
}
```

**Testing:**
```python
# Checks esenciales
assert df.count() > 0  # Row count
assert df.select('id').distinct().count() == df.count()  # Uniqueness
```

### 80% de PrÃ¡ctica (Projects & Repetition)

- âœ… Build el mismo pipeline con 3 datasets diferentes
- âœ… Cargas incrementales diarias (misma lÃ³gica, distintas fechas)
- âœ… Refactoring de cÃ³digo (mejorar pipeline existente)
- âœ… Debugging sessions (arreglar pipelines rotos)

---

## ğŸš¦ Getting Started

### Prerequisitos

**Conocimientos:**
- âœ… ProgramaciÃ³n bÃ¡sica (Python o Java/Scala deseable)
- âœ… SQL bÃ¡sico (SELECT, WHERE, JOINs simples)
- âœ… LÃ­nea de comandos (bash, terminal)
- âœ… Git bÃ¡sico (clone, commit, push)

**Herramientas:**
- ğŸ’» Laptop con 8GB RAM mÃ­nimo (16GB recomendado)
- ğŸ³ Docker Desktop
- ğŸ”§ Git + GitHub account
- ğŸ“ VSCode (o IntelliJ IDEA para Scala)
- â˜ï¸ Databricks Community Edition (gratis)
- ğŸ˜ PostgreSQL (via Docker)

### InstalaciÃ³n

Ver **[MÃ³dulo 00: Plan & Setup](00-plan-setup/README.md)** para instrucciones detalladas.

```bash
# RÃ¡pido check de herramientas
git --version
docker --version
python3 --version

# Levantar PostgreSQL
docker run --name postgres-pareto \
  -e POSTGRES_USER=dataeng \
  -e POSTGRES_PASSWORD=pareto2080 \
  -e POSTGRES_DB=learning_db \
  -p 5432:5432 \
  -d postgres:15
```

### Ritmo Recomendado

| Ritmo | Horas/semana | DuraciÃ³n total |
|-------|--------------|----------------|
| **Intensivo** | 20-25h | 6-8 semanas |
| **Regular** | 15-20h | 8-10 semanas |
| **Part-time** | 10-15h | 12-14 semanas |

---

## ğŸ“Š Acceptance Criteria

Checklist completo para validar que el curso estÃ¡ listo:

### Module Structure
- [x] 13 mÃ³dulos creados (00-12)
- [x] Cada mÃ³dulo tiene los 5 archivos (README, actividad, progreso, retroalimentacion, recursos)
- [ ] NingÃºn archivo estÃ¡ vacÃ­o (in progress)

### Content Quality
- [x] Cada `actividad-interactiva.md` debe contener comandos verificables
- [x] Cada ejercicio debe tener campos para respuestas (`__________`) y verificaciÃ³n
- [ ] Al menos 6 mini-proyectos + 1 proyecto final (in progress)

### Helper Artifacts
- [x] Airflow DAG skeleton (Python)
- [x] Spark Scala job example
- [x] SQL DDL examples (fact/dimension tables)
- [x] SQL test examples (uniqueness, not_null)

### Projects
- [x] Module 02: SQL reporting queries (planned)
- [x] Module 03: Python ETL pipeline (planned)
- [x] Module 04: Spark Scala cleaning job (planned)
- [x] Module 06: Delta Lake SCD Type 2 (planned)
- [x] Module 07: dbt data mart (planned)
- [x] Module 08: Airflow DAG orchestration (planned)
- [x] Module 09: Testing suite (planned)
- [x] Module 12: End-to-end integrator (planned)

### Validation
- [x] Sanity check script created (`sanity_check.sh`)
- [ ] All modules passing sanity checks (in progress)

---

## ğŸ”§ Sanity Check

Ejecutar script de validaciÃ³n:

```bash
./sanity_check.sh
```

Este script verifica:
- âœ… Todos los mÃ³dulos existen
- âœ… Cada mÃ³dulo tiene los 5 archivos requeridos
- âœ… Archivos no estÃ¡n vacÃ­os
- âœ… Helper artifacts existen
- âœ… Conteo de proyectos >= 6

---

## ğŸ“ Expected Outcomes

Al finalizar este curso serÃ¡s capaz de:

âœ… **Construir pipelines de producciÃ³n** con SQL, Spark (Scala), Databricks y Airflow  
âœ… **Implementar cargas incrementales idempotentes** con particionado eficiente  
âœ… **Escribir tests de calidad de datos** (row counts, uniqueness, null checks)  
âœ… **Orquestar workflows complejos** con Airflow (retries, backfills)  
âœ… **Versionar cÃ³digo en GitHub** con CI/CD bÃ¡sico  
âœ… **Optimizar costos y performance** en ambientes cloud  
âœ… **Debuggear pipelines rotos** con logs y mÃ©tricas  
âœ… **Presentar proyectos tÃ©cnicos** con documentaciÃ³n profesional

---

## ğŸ“ Portfolio Projects

Al completar el curso tendrÃ¡s **8+ proyectos** en GitHub:

1. **SQL Reporting Queries** â€” anÃ¡lisis de ventas con window functions
2. **Python ETL Pipeline** â€” CSV â†’ PostgreSQL con validaciones
3. **Spark Scala Data Cleaning** â€” procesamiento distribuido con particionado
4. **Databricks Production Job** â€” notebook productivizado
5. **Delta Lake SCD Type 2** â€” historizaciÃ³n de dimensiones
6. **dbt Data Mart** â€” transformaciones SQL versionadas
7. **Airflow DAG Orchestration** â€” pipeline completo orquestado
8. **End-to-End Integration** â€” CSV/API â†’ Spark â†’ Delta â†’ dbt â†’ Tests â†’ Airflow

---

## ğŸ” Security Note

- **Nunca incluir credenciales reales** en cÃ³digo
- Usar placeholders: `<YOUR_ACCOUNT>`, `<YOUR_KEY>`, `<YOUR_TOKEN>`
- Guardar secretos en **GitHub Secrets**, **Vault**, o archivos `.env` (gitignored)

---

## ğŸ“š Additional Resources

**DocumentaciÃ³n Oficial:**
- [Apache Spark](https://spark.apache.org/docs/latest/)
- [Apache Airflow](https://airflow.apache.org/docs/)
- [Databricks](https://docs.databricks.com/)
- [PostgreSQL](https://www.postgresql.org/docs/)
- [dbt](https://docs.getdbt.com/)

**Comunidades:**
- Reddit: [r/dataengineering](https://www.reddit.com/r/dataengineering/)
- Discord: Data Engineering Community
- Slack: dbt Community, Airflow Community

---

## ğŸ¤ Contributing

Mejoras y contribuciones son bienvenidas!

1. Fork el repositorio
2. Crea rama feature (`git checkout -b feature/mejora-modulo-x`)
3. Commit cambios (`git commit -m 'Add: mejora en mÃ³dulo X'`)
4. Push a la rama (`git push origin feature/mejora-modulo-x`)
5. Abre Pull Request

---

## ğŸ“… Roadmap

**v1.0 (Actual):**
- âœ… Course blueprint
- âœ… MÃ³dulos 00-01 completos
- âœ… Helper artifacts
- âœ… Sanity check script

**v1.1 (PrÃ³ximo):**
- [ ] MÃ³dulos 02, 04, 08, 12 con contenido completo
- [ ] Datasets de ejemplo descargables
- [ ] Videos complementarios (opcional)

**v2.0 (Futuro):**
- [ ] Todos los mÃ³dulos completos
- [ ] CI/CD examples con GitHub Actions
- [ ] Terraform templates
- [ ] Great Expectations integration

---

## ğŸ“„ License

Este curso es de cÃ³digo abierto para fines educativos.

---

## âœï¸ Autor

**Data Engineering Pareto 20/80 Course**  
Creado con la metodologÃ­a Pareto: Maximize learning, minimize noise.

ğŸ“… **Ãšltima actualizaciÃ³n:** 2024-01-01  
ğŸ“ **VersiÃ³n:** 1.0 (MVP)

---

**ğŸ’¡ Recuerda:** El principio Pareto no significa ignorar el 80% restante. Significa **priorizar inteligentemente** para maximizar el ROI de tu tiempo de estudio.

**ğŸš€ Â¡Comienza ahora!** â†’ [MÃ³dulo 00: Plan & Setup](00-plan-setup/README.md)
