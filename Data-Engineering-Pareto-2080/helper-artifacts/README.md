# üõ†Ô∏è Helper Artifacts ‚Äî Data Engineering Pareto 20/80

Este directorio contiene **code snippets y templates reutilizables** que puedes copiar y adaptar para tus propios proyectos.

---

## üìÅ Contenido

### 1. Airflow DAG Skeleton (`airflow/`)

**Archivo:** `minimal_dag_skeleton.py`

**Qu√© incluye:**
- Template b√°sico de DAG con best practices
- Ejemplos de PythonOperator, BashOperator, FileSensor
- Configuraci√≥n de retries, timeouts, scheduling
- Uso de XCom para pasar datos entre tasks
- Comentarios explicativos del 20% core

**Uso:**
```bash
# Copiar a tu directorio de Airflow
cp helper-artifacts/airflow/minimal_dag_skeleton.py ~/airflow-pareto/dags/mi_pipeline_dag.py

# Editar y personalizar
# Ejecutar en Airflow
```

---

### 2. Spark Scala Job (`spark-scala/`)

**Archivo:** `DataCleaningJob.scala`

**Qu√© incluye:**
- Job completo de limpieza de datos
- Lectura de CSV/Parquet
- Transformaciones core (filter, select, groupBy, join)
- Deduplicaci√≥n con window functions
- Escritura particionada
- Data quality checks integrados
- Best practices y anti-patterns explicados

**Compilaci√≥n:**
```bash
# Asumiendo sbt configurado
sbt package

# Submit local
spark-submit \
  --class com.dataeng.pareto.DataCleaningJob \
  --master local[*] \
  target/scala-2.12/spark-job.jar \
  data/input/sales.csv \
  data/output/sales_clean
```

**Uso en Databricks:**
1. Crear notebook Scala
2. Copiar c√≥digo del job
3. Ajustar paths (usar DBFS)
4. Ejecutar en cluster

---

### 3. SQL DDL Examples (`sql/`)

**Archivo:** `ddl_fact_dimension_tables.sql`

**Qu√© incluye:**
- Dimension tables (dim_customer, dim_product, dim_date, dim_location)
- Fact tables (fact_sales, fact_inventory)
- Slowly Changing Dimensions (SCD Type 1 y Type 2)
- Aggregate tables
- Staging tables
- Views √∫tiles
- Indexes y constraints
- Partitioning strategies

**Uso:**
```bash
# Ejecutar en PostgreSQL
psql -U dataeng -d warehouse_db -f helper-artifacts/sql/ddl_fact_dimension_tables.sql

# O en chunks
psql -U dataeng -d warehouse_db << EOF
-- Copy paste secciones espec√≠ficas
CREATE TABLE dim_customer (...);
EOF
```

---

### 4. SQL Data Quality Tests (`sql/`)

**Archivo:** `data_quality_tests.sql`

**Qu√© incluye:**
- Tests de Completeness (row count, nulls, missing data)
- Tests de Uniqueness (duplicates)
- Tests de Validity (ranges, formats, foreign keys)
- Tests de Consistency (calculated fields, cross-table)
- Tests de Timeliness (freshness)
- Tests de Outliers (statistical)
- Funci√≥n PL/pgSQL para test suite completo

**Uso:**
```bash
# Ejecutar tests individuales
psql -U dataeng -d warehouse_db -f helper-artifacts/sql/data_quality_tests.sql

# O ejecutar suite completa
psql -U dataeng -d warehouse_db -c "SELECT * FROM run_all_data_quality_tests();"

# Integrar en Airflow
# Ver airflow/minimal_dag_skeleton.py para ejemplo de task de validaci√≥n
```

---

## üéØ Pareto 20% ‚Äî C√≥mo Usar Estos Artifacts

### 1. No copies ciegamente
- **Entiende** cada l√≠nea antes de copiar
- **Adapta** a tus necesidades espec√≠ficas
- **Simplifica** si no necesitas toda la funcionalidad

### 2. Orden de uso recomendado
1. **Primero:** SQL DDL ‚Äî crea tu warehouse schema
2. **Segundo:** SQL Tests ‚Äî valida tus datos
3. **Tercero:** Spark job ‚Äî procesa y carga datos
4. **Cuarto:** Airflow DAG ‚Äî orquesta todo el pipeline

### 3. Integraci√≥n t√≠pica

```
Pipeline completo:

1. Airflow DAG trigger
   ‚Üì
2. Extract task (Python/Bash)
   ‚Üì
3. Spark job (limpieza y transformaci√≥n)
   ‚Üì
4. Load to warehouse (COPY/INSERT)
   ‚Üì
5. SQL tests (data quality)
   ‚Üì
6. Alertas si falla
   ‚Üì
7. Success notification
```

### 4. Customizaci√≥n por proyecto

**Para proyecto simple (CSV ‚Üí PostgreSQL):**
- Usar solo: Airflow DAG b√°sico + SQL tests
- Skip: Spark (usar pandas en Python)

**Para proyecto medium (ETL distribuido):**
- Usar todo: Airflow + Spark + SQL DDL + Tests

**Para proyecto enterprise (data lakehouse):**
- Extender Spark job con Delta Lake
- A√±adir dbt para transformaciones
- Integrar Great Expectations para tests avanzados

---

## üìö Templates Adicionales (Futuros)

Planeados para a√±adir:

- [ ] `databricks/job_config.json` - Configuraci√≥n de Databricks Job
- [ ] `dbt/models/` - Ejemplos de modelos dbt
- [ ] `great_expectations/` - Config de Great Expectations
- [ ] `terraform/` - IaC para desplegar infraestructura
- [ ] `docker/` - Dockerfiles para servicios locales
- [ ] `testing/pytest_examples.py` - Tests unitarios Python

---

## üîß Troubleshooting

### Airflow DAG no aparece en UI
```bash
# Verificar syntax
python helper-artifacts/airflow/minimal_dag_skeleton.py

# Verificar logs
docker logs airflow-standalone | grep ERROR
```

### Spark job falla con OutOfMemory
```bash
# Aumentar memoria del executor
spark-submit \
  --executor-memory 4G \
  --driver-memory 2G \
  ...
```

### SQL tests muy lentos
```sql
-- A√±adir indexes en columnas usadas en WHERE
CREATE INDEX idx_fact_sales_date ON fact_sales(date_key);

-- Usar sampling para tables grandes
SELECT ... FROM fact_sales TABLESAMPLE SYSTEM (10);  -- 10% sample
```

---

## üí° Best Practices

1. **Versionado:**
   - Guardar estos templates en tu repo Git
   - Documentar cambios que haces

2. **Reusabilidad:**
   - Crear tu propia librer√≠a de snippets
   - Estandarizar dentro de tu equipo

3. **Testing:**
   - Testear localmente antes de deploy
   - Usar datos sint√©ticos para dev

4. **Documentaci√≥n:**
   - Comentar tus adaptaciones
   - Mantener README actualizado

---

## üöÄ Next Steps

1. **Explorar** cada archivo en detalle
2. **Ejecutar** ejemplos en tu entorno local
3. **Adaptar** para tu primer proyecto
4. **Contribuir** mejoras a este repo (pull requests bienvenidos!)

---

**üìù Nota:** Estos templates son punto de partida, no soluci√≥n final. Siempre optimiza seg√∫n tu caso de uso espec√≠fico.

**üéì Curso:** Data Engineering Pareto 20/80  
**üìÖ √öltima actualizaci√≥n:** 2024-01-01
