# ğŸ¯ Plan de Estudio â€” Data Engineering Pareto 20/80 (Projects-First)

## Resumen Ejecutivo

Este curso prÃ¡ctico sigue el **principio de Pareto**: identifica el **20% de conocimientos** que generan el **80% del impacto** profesional en Data Engineering. Prioriza proyectos reales, ejercicios verificables y herramientas de producciÃ³n: **SQL, ETL, Testing, Spark (Scala), Databricks, Airflow y GitHub**.

**Nivel:** Principiante a Intermedio  
**DuraciÃ³n:** 10-12 semanas (12-15 horas/semana)  
**Modalidad:** Auto-guiado, projects-first, verificable  
**Proyecto Final:** Pipeline end-to-end con orquestaciÃ³n Airflow

---

## ğŸ¯ Objetivos de Aprendizaje

Al completar este roadmap dominarÃ¡s el **20% crÃ­tico** que te hace job-ready:

1. **SQL avanzado**: JOINs, window functions, MERGE/UPSERT, deduplicaciÃ³n
2. **ETL idempotente**: cargas incrementales, particionado, manejo de errores
3. **Spark (Scala)**: transformaciones DataFrame, escritura particionada, optimizaciÃ³n
4. **Databricks**: notebooks, jobs, cluster config, Delta Lake
5. **Airflow**: diseÃ±o de DAGs, retries, backfills, parametrizaciÃ³n
6. **Testing**: row counts, uniqueness, null checks, smoke tests end-to-end
7. **GitHub**: versionado de cÃ³digo, CI/CD bÃ¡sico para pipelines

---

## ğŸ§  Principio Pareto Aplicado

### 20% de Conocimientos (High-Impact Core)

**SQL:**
- JOINs (INNER, LEFT, FULL OUTER)
- Window functions (ROW_NUMBER, RANK, LAG/LEAD)
- GROUP BY agregaciones
- MERGE/UPSERT statements
- CTEs (Common Table Expressions)

**ETL:**
- Cargas incrementales idempotentes
- Particionado por fecha/regiÃ³n
- DeduplicaciÃ³n (ROW_NUMBER, QUALIFY)
- Manejo de errores y reintentos

**Spark/Scala:**
- DataFrame transforms (select, filter, groupBy, join)
- Escritura particionada (partitionBy)
- Evitar collect(), preferir actions eficientes
- mapPartitions para transformaciones complejas

**OrquestaciÃ³n:**
- DiseÃ±o de DAGs con dependencias
- Retries y backfills
- ParametrizaciÃ³n (variables, Jinja templates)
- Sensores para esperar datos

**Testing:**
- Row counts esperados
- Checks de unicidad (PRIMARY KEY)
- ValidaciÃ³n NOT NULL
- Smoke test end-to-end

### 80% de PrÃ¡ctica (Projects & Repetition)

- **Repetir el mismo pipeline con 3 datasets diferentes** (reinforcement)
- **Build incrementales diarios** (misma lÃ³gica, distintas fechas)
- **Refactoring de cÃ³digo** (mejorar pipeline existente)
- **Debugging sessions** (arreglar pipelines rotos)
- **Code reviews** (evaluar cÃ³digo de ejemplo)

---

## ğŸ“š Estructura del Curso (12 MÃ³dulos)

### **MÃ³dulo 00: Plan & Setup**
- ğŸ¯ ConfiguraciÃ³n del entorno (Docker, Git, Databricks Community)
- ğŸ› ï¸ Herramientas: Spark local, Airflow, PostgreSQL
- ğŸ“Š Datasets de prÃ¡ctica (CSV, API)
- â±ï¸ DuraciÃ³n: 3-4 horas

### **MÃ³dulo 01: IntroducciÃ³n al Pareto 20/80**
- ğŸ¯ QuÃ© enfocar y cÃ³mo estudiar
- ğŸ¯ MetodologÃ­a projects-first
- ğŸ¯ Roadmap de carrera Data Engineer
- â±ï¸ DuraciÃ³n: 2 horas

### **MÃ³dulo 02: SQL Core**
- ğŸ¯ JOINs, window functions, GROUP BY
- ğŸ¯ MERGE/UPSERT, deduplicaciÃ³n
- ğŸ“¦ **Proyecto:** Set de reporting queries sobre ventas
- â±ï¸ DuraciÃ³n: 10-12 horas

### **MÃ³dulo 03: Python ETL Basics**
- ğŸ¯ Ingesta desde CSV y APIs
- ğŸ¯ pandas transformations, write to PostgreSQL
- ğŸ“¦ **Proyecto:** ETL simple con validaciones
- â±ï¸ DuraciÃ³n: 8-10 horas

### **MÃ³dulo 04: Spark Scala Fundamentals**
- ğŸ¯ DataFrame API, transformaciones, actions
- ğŸ¯ Particionado, escritura eficiente
- ğŸ“¦ **Proyecto:** Job Spark para limpieza y particionado
- â±ï¸ DuraciÃ³n: 12-15 horas

### **MÃ³dulo 05: Databricks Workflow**
- ğŸ¯ Notebooks, jobs, cluster config
- ğŸ¯ IntegraciÃ³n con Git
- ğŸ“¦ **Proyecto:** Job productivo en Databricks
- â±ï¸ DuraciÃ³n: 8-10 horas

### **MÃ³dulo 06: Delta Lake & Storage**
- ğŸ¯ ACID, upserts, time travel
- ğŸ¯ OptimizaciÃ³n (OPTIMIZE, Z-ORDER)
- ğŸ“¦ **Proyecto:** Tabla Delta con SCD Type 2
- â±ï¸ DuraciÃ³n: 8-10 horas

### **MÃ³dulo 07: DBT or Transforms**
- ğŸ¯ Modelos SQL sobre Delta
- ğŸ¯ Testing integrado, documentaciÃ³n
- ğŸ“¦ **Proyecto:** Data mart dimensional con dbt
- â±ï¸ DuraciÃ³n: 10-12 horas

### **MÃ³dulo 08: Airflow Orchestration**
- ğŸ¯ DAGs, operators, sensores
- ğŸ¯ Scheduling, retries, backfills
- ğŸ“¦ **Proyecto:** DAG orquestando pipeline completo
- â±ï¸ DuraciÃ³n: 12-15 horas

### **MÃ³dulo 09: Testing & Data Quality**
- ğŸ¯ Great Expectations / SQL checks
- ğŸ¯ Tests unitarios, integraciÃ³n
- ğŸ“¦ **Proyecto:** Suite de tests para pipeline
- â±ï¸ DuraciÃ³n: 8-10 horas

### **MÃ³dulo 10: Observability & Cost**
- ğŸ¯ Logging estructurado, mÃ©tricas
- ğŸ¯ Monitoreo de SLAs
- ğŸ“¦ **Proyecto:** Dashboard de monitoreo
- â±ï¸ DuraciÃ³n: 6-8 horas

### **MÃ³dulo 11: Security & Governance**
- ğŸ¯ Manejo de secretos (Vault, GitHub Secrets)
- ğŸ¯ RLS (Row-Level Security), encriptaciÃ³n
- ğŸ“¦ **Proyecto:** Pipeline con credenciales seguras
- â±ï¸ DuraciÃ³n: 4-6 horas

### **MÃ³dulo 12: Final Project**
- ğŸ¯ **End-to-end pipeline integrador:**
  - Ingesta: CSV + API â†’ raw landing
  - Transform: Spark (Scala) â†’ Delta tables
  - Warehouse: dbt models â†’ analytical tables
  - Orchestration: Airflow DAG
  - Testing: Great Expectations
  - Monitoring: logs & metrics
- ğŸ“¦ **Entregables:** cÃ³digo en GitHub, documentaciÃ³n, presentaciÃ³n
- â±ï¸ DuraciÃ³n: 20-25 horas

---

## ğŸ“ Formato de cada MÃ³dulo

Cada mÃ³dulo contiene **5 archivos estandarizados**:

1. **README.md**: objetivos, teorÃ­a, actividades, entregables
2. **actividad-interactiva.md**: ejercicios con comandos verificables, campos para respuestas, duraciones
3. **progreso.md**: checklist de avance
4. **retroalimentacion.md**: rÃºbrica de evaluaciÃ³n con porcentajes
5. **recursos.md**: datasets, links, snippets de cÃ³digo

---

## ğŸ› ï¸ Prerequisitos

### Conocimientos Previos
- âœ… ProgramaciÃ³n bÃ¡sica (Python o Java/Scala deseable)
- âœ… SQL bÃ¡sico (SELECT, WHERE, JOINs simples)
- âœ… LÃ­nea de comandos (bash, terminal)
- âœ… Git bÃ¡sico (clone, commit, push)

### Herramientas Necesarias
- ğŸ’» Laptop con 8GB RAM mÃ­nimo (16GB recomendado)
- ğŸ³ Docker Desktop
- ğŸ”§ Git + GitHub account
- ğŸ“ VSCode (o IntelliJ IDEA para Scala)
- â˜ï¸ Databricks Community Edition (gratis)
- ğŸ˜ PostgreSQL (via Docker)

---

## ğŸš€ CÃ³mo Usar este Roadmap

### Paso 1: ConfiguraciÃ³n Inicial
```bash
# Clonar el repositorio
git clone https://github.com/angra8410/all-my-learnings.git
cd all-my-learnings/Data-Engineering-Pareto-2080

# Crear rama de trabajo personal
git checkout -b feature/mi-progreso-pareto
```

### Paso 2: Seguir Orden Secuencial
1. Lee **README.md** del mÃ³dulo
2. Completa **actividad-interactiva.md** (hands-on)
3. Marca **progreso.md**
4. AutoevalÃºate con **retroalimentacion.md**
5. Profundiza con **recursos.md**

### Paso 3: Ritmo Recomendado
- **Intensivo:** 2 mÃ³dulos/semana (25+ horas/semana) â†’ 6 semanas
- **Regular:** 1.5 mÃ³dulos/semana (15-20 horas/semana) â†’ 8 semanas
- **Part-time:** 1 mÃ³dulo/semana (12-15 horas/semana) â†’ 12 semanas

---

## ğŸ¯ Acceptance Criteria (Course Complete Checklist)

### Module Structure
- [ ] 13 mÃ³dulos creados (00-12)
- [ ] Cada mÃ³dulo tiene los 5 archivos (README, actividad, progreso, retroalimentacion, recursos)
- [ ] NingÃºn archivo estÃ¡ vacÃ­o

### Content Quality
- [ ] Cada `actividad-interactiva.md` contiene comandos verificables
- [ ] Cada ejercicio tiene campos para respuestas (__________) y verificaciÃ³n
- [ ] Al menos 6 mini-proyectos + 1 proyecto final

### Helper Artifacts
- [ ] Airflow DAG skeleton (Python)
- [ ] Spark Scala job example
- [ ] SQL DDL examples (fact/dimension tables)
- [ ] SQL test examples (uniqueness, not_null)
- [ ] Databricks CLI commands

### Projects
- [ ] Module 02: SQL reporting queries
- [ ] Module 03: Python ETL pipeline
- [ ] Module 04: Spark Scala cleaning job
- [ ] Module 06: Delta Lake SCD Type 2
- [ ] Module 07: dbt data mart
- [ ] Module 08: Airflow DAG orchestration
- [ ] Module 09: Testing suite
- [ ] Module 12: End-to-end integrator

---

## ğŸ“Š Expected Outcomes

Al finalizar este curso **projects-first Pareto 20/80**, serÃ¡s capaz de:

âœ… **Construir pipelines de producciÃ³n** con SQL, Spark (Scala), Databricks y Airflow  
âœ… **Implementar cargas incrementales idempotentes** con particionado eficiente  
âœ… **Escribir tests de calidad de datos** (row counts, uniqueness, null checks)  
âœ… **Orquestar workflows complejos** con Airflow (retries, backfills)  
âœ… **Versionar cÃ³digo en GitHub** con CI/CD bÃ¡sico  
âœ… **Optimizar costos y performance** en ambientes cloud  
âœ… **Debuggear pipelines rotos** con logs y mÃ©tricas  
âœ… **Presentar proyectos tÃ©cnicos** con documentaciÃ³n profesional

---

## ğŸ” Security Note

- **Nunca incluir credenciales reales** en cÃ³digo
- Usar placeholders: `<YOUR_ACCOUNT>`, `<YOUR_KEY>`, `<YOUR_TOKEN>`
- Guardar secretos en **GitHub Secrets**, **Vault**, o archivos `.env` (gitignored)
- Revisar `.gitignore` antes de cada commit

---

## ğŸ“ Next Steps (After Course Completion)

1. **Deploy to production:** migrar proyecto final a AWS/Azure/GCP
2. **Add CI/CD:** GitHub Actions para tests automÃ¡ticos
3. **Contribute to OSS:** colaborar en proyectos Airflow, dbt, Spark
4. **Build portfolio:** documentar proyectos en LinkedIn/blog tÃ©cnico
5. **Apply to jobs:** usar proyectos como portfolio en entrevistas

---

**ğŸ“ Creado con la metodologÃ­a Pareto 20/80 â€” Maximize learning, minimize noise.**
